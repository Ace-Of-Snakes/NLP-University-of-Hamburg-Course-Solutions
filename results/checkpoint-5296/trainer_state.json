{
  "best_metric": 0.3861248195171356,
  "best_model_checkpoint": "./results\\checkpoint-5296",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 5296,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0037764350453172208,
      "grad_norm": 11.012073516845703,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.6663,
      "step": 10
    },
    {
      "epoch": 0.0075528700906344415,
      "grad_norm": 4.421233654022217,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.6623,
      "step": 20
    },
    {
      "epoch": 0.011329305135951661,
      "grad_norm": 4.972522735595703,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.689,
      "step": 30
    },
    {
      "epoch": 0.015105740181268883,
      "grad_norm": 4.239871501922607,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.6794,
      "step": 40
    },
    {
      "epoch": 0.0188821752265861,
      "grad_norm": 5.105374813079834,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6726,
      "step": 50
    },
    {
      "epoch": 0.022658610271903322,
      "grad_norm": 4.179624557495117,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.6848,
      "step": 60
    },
    {
      "epoch": 0.026435045317220542,
      "grad_norm": 4.666784763336182,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.6648,
      "step": 70
    },
    {
      "epoch": 0.030211480362537766,
      "grad_norm": 5.113445281982422,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.6964,
      "step": 80
    },
    {
      "epoch": 0.033987915407854986,
      "grad_norm": 3.01875901222229,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 0.6889,
      "step": 90
    },
    {
      "epoch": 0.0377643504531722,
      "grad_norm": 2.586751699447632,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6515,
      "step": 100
    },
    {
      "epoch": 0.04154078549848943,
      "grad_norm": 2.793365716934204,
      "learning_rate": 4.4e-06,
      "loss": 0.6171,
      "step": 110
    },
    {
      "epoch": 0.045317220543806644,
      "grad_norm": 3.8521790504455566,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.7052,
      "step": 120
    },
    {
      "epoch": 0.04909365558912387,
      "grad_norm": 3.9442811012268066,
      "learning_rate": 5.2e-06,
      "loss": 0.7169,
      "step": 130
    },
    {
      "epoch": 0.052870090634441085,
      "grad_norm": 3.207753896713257,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.6765,
      "step": 140
    },
    {
      "epoch": 0.05664652567975831,
      "grad_norm": 2.578266143798828,
      "learning_rate": 6e-06,
      "loss": 0.6877,
      "step": 150
    },
    {
      "epoch": 0.06042296072507553,
      "grad_norm": 3.6592695713043213,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.6969,
      "step": 160
    },
    {
      "epoch": 0.06419939577039276,
      "grad_norm": 2.7121047973632812,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.6596,
      "step": 170
    },
    {
      "epoch": 0.06797583081570997,
      "grad_norm": 3.595581531524658,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.6751,
      "step": 180
    },
    {
      "epoch": 0.07175226586102719,
      "grad_norm": 1.8962959051132202,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.6871,
      "step": 190
    },
    {
      "epoch": 0.0755287009063444,
      "grad_norm": 3.378194808959961,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.6795,
      "step": 200
    },
    {
      "epoch": 0.07930513595166164,
      "grad_norm": 2.0000531673431396,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.6867,
      "step": 210
    },
    {
      "epoch": 0.08308157099697885,
      "grad_norm": 2.455225944519043,
      "learning_rate": 8.8e-06,
      "loss": 0.6963,
      "step": 220
    },
    {
      "epoch": 0.08685800604229607,
      "grad_norm": 2.0981061458587646,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.6723,
      "step": 230
    },
    {
      "epoch": 0.09063444108761329,
      "grad_norm": 1.9387165307998657,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.6603,
      "step": 240
    },
    {
      "epoch": 0.09441087613293052,
      "grad_norm": 3.146921396255493,
      "learning_rate": 1e-05,
      "loss": 0.6608,
      "step": 250
    },
    {
      "epoch": 0.09818731117824774,
      "grad_norm": 2.5353615283966064,
      "learning_rate": 1.04e-05,
      "loss": 0.6722,
      "step": 260
    },
    {
      "epoch": 0.10196374622356495,
      "grad_norm": 2.031273126602173,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 0.6749,
      "step": 270
    },
    {
      "epoch": 0.10574018126888217,
      "grad_norm": 3.1785271167755127,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.6601,
      "step": 280
    },
    {
      "epoch": 0.1095166163141994,
      "grad_norm": 7.288255214691162,
      "learning_rate": 1.16e-05,
      "loss": 0.6616,
      "step": 290
    },
    {
      "epoch": 0.11329305135951662,
      "grad_norm": 12.429780006408691,
      "learning_rate": 1.2e-05,
      "loss": 0.6374,
      "step": 300
    },
    {
      "epoch": 0.11706948640483383,
      "grad_norm": 7.315486431121826,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 0.6547,
      "step": 310
    },
    {
      "epoch": 0.12084592145015106,
      "grad_norm": 7.235942840576172,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.6365,
      "step": 320
    },
    {
      "epoch": 0.12462235649546828,
      "grad_norm": 12.20388126373291,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 0.603,
      "step": 330
    },
    {
      "epoch": 0.1283987915407855,
      "grad_norm": 5.833060264587402,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.6319,
      "step": 340
    },
    {
      "epoch": 0.13217522658610273,
      "grad_norm": 11.05210018157959,
      "learning_rate": 1.4e-05,
      "loss": 0.6243,
      "step": 350
    },
    {
      "epoch": 0.13595166163141995,
      "grad_norm": 9.59464168548584,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.5329,
      "step": 360
    },
    {
      "epoch": 0.13972809667673716,
      "grad_norm": 10.83176040649414,
      "learning_rate": 1.48e-05,
      "loss": 0.5327,
      "step": 370
    },
    {
      "epoch": 0.14350453172205438,
      "grad_norm": 7.718207359313965,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.4781,
      "step": 380
    },
    {
      "epoch": 0.1472809667673716,
      "grad_norm": 12.009187698364258,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 0.6327,
      "step": 390
    },
    {
      "epoch": 0.1510574018126888,
      "grad_norm": 11.404032707214355,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.5435,
      "step": 400
    },
    {
      "epoch": 0.15483383685800603,
      "grad_norm": 7.329256534576416,
      "learning_rate": 1.64e-05,
      "loss": 0.6337,
      "step": 410
    },
    {
      "epoch": 0.15861027190332327,
      "grad_norm": 5.938558578491211,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.5379,
      "step": 420
    },
    {
      "epoch": 0.1623867069486405,
      "grad_norm": 9.722161293029785,
      "learning_rate": 1.72e-05,
      "loss": 0.5659,
      "step": 430
    },
    {
      "epoch": 0.1661631419939577,
      "grad_norm": 11.274197578430176,
      "learning_rate": 1.76e-05,
      "loss": 0.485,
      "step": 440
    },
    {
      "epoch": 0.16993957703927492,
      "grad_norm": 9.283723831176758,
      "learning_rate": 1.8e-05,
      "loss": 0.5168,
      "step": 450
    },
    {
      "epoch": 0.17371601208459214,
      "grad_norm": 7.6195387840271,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.6166,
      "step": 460
    },
    {
      "epoch": 0.17749244712990936,
      "grad_norm": 7.249781131744385,
      "learning_rate": 1.88e-05,
      "loss": 0.5142,
      "step": 470
    },
    {
      "epoch": 0.18126888217522658,
      "grad_norm": 4.847316741943359,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.5342,
      "step": 480
    },
    {
      "epoch": 0.18504531722054382,
      "grad_norm": 4.915506839752197,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.5824,
      "step": 490
    },
    {
      "epoch": 0.18882175226586104,
      "grad_norm": 4.316428184509277,
      "learning_rate": 2e-05,
      "loss": 0.5516,
      "step": 500
    },
    {
      "epoch": 0.19259818731117825,
      "grad_norm": 7.426794528961182,
      "learning_rate": 1.9984301412872844e-05,
      "loss": 0.5088,
      "step": 510
    },
    {
      "epoch": 0.19637462235649547,
      "grad_norm": 9.683324813842773,
      "learning_rate": 1.9968602825745684e-05,
      "loss": 0.5426,
      "step": 520
    },
    {
      "epoch": 0.2001510574018127,
      "grad_norm": 7.149803638458252,
      "learning_rate": 1.9952904238618527e-05,
      "loss": 0.484,
      "step": 530
    },
    {
      "epoch": 0.2039274924471299,
      "grad_norm": 7.291558265686035,
      "learning_rate": 1.9937205651491366e-05,
      "loss": 0.4806,
      "step": 540
    },
    {
      "epoch": 0.20770392749244712,
      "grad_norm": 4.934969902038574,
      "learning_rate": 1.992150706436421e-05,
      "loss": 0.5226,
      "step": 550
    },
    {
      "epoch": 0.21148036253776434,
      "grad_norm": 5.913609504699707,
      "learning_rate": 1.990580847723705e-05,
      "loss": 0.4928,
      "step": 560
    },
    {
      "epoch": 0.21525679758308158,
      "grad_norm": 5.40029764175415,
      "learning_rate": 1.989010989010989e-05,
      "loss": 0.5289,
      "step": 570
    },
    {
      "epoch": 0.2190332326283988,
      "grad_norm": 4.773771286010742,
      "learning_rate": 1.9874411302982734e-05,
      "loss": 0.5058,
      "step": 580
    },
    {
      "epoch": 0.22280966767371602,
      "grad_norm": 6.69830846786499,
      "learning_rate": 1.9858712715855573e-05,
      "loss": 0.4663,
      "step": 590
    },
    {
      "epoch": 0.22658610271903323,
      "grad_norm": 3.9953970909118652,
      "learning_rate": 1.9843014128728416e-05,
      "loss": 0.4665,
      "step": 600
    },
    {
      "epoch": 0.23036253776435045,
      "grad_norm": 8.058004379272461,
      "learning_rate": 1.982731554160126e-05,
      "loss": 0.4801,
      "step": 610
    },
    {
      "epoch": 0.23413897280966767,
      "grad_norm": 6.624642848968506,
      "learning_rate": 1.98116169544741e-05,
      "loss": 0.4784,
      "step": 620
    },
    {
      "epoch": 0.23791540785498488,
      "grad_norm": 4.124952793121338,
      "learning_rate": 1.979591836734694e-05,
      "loss": 0.4824,
      "step": 630
    },
    {
      "epoch": 0.24169184290030213,
      "grad_norm": 8.022481918334961,
      "learning_rate": 1.9780219780219784e-05,
      "loss": 0.5676,
      "step": 640
    },
    {
      "epoch": 0.24546827794561935,
      "grad_norm": 4.932467937469482,
      "learning_rate": 1.9764521193092623e-05,
      "loss": 0.4421,
      "step": 650
    },
    {
      "epoch": 0.24924471299093656,
      "grad_norm": 11.081498146057129,
      "learning_rate": 1.9748822605965466e-05,
      "loss": 0.5498,
      "step": 660
    },
    {
      "epoch": 0.25302114803625375,
      "grad_norm": 5.544564247131348,
      "learning_rate": 1.9733124018838306e-05,
      "loss": 0.4937,
      "step": 670
    },
    {
      "epoch": 0.256797583081571,
      "grad_norm": 6.695540428161621,
      "learning_rate": 1.971742543171115e-05,
      "loss": 0.5276,
      "step": 680
    },
    {
      "epoch": 0.26057401812688824,
      "grad_norm": 7.185750961303711,
      "learning_rate": 1.9701726844583988e-05,
      "loss": 0.5228,
      "step": 690
    },
    {
      "epoch": 0.26435045317220546,
      "grad_norm": 3.9795148372650146,
      "learning_rate": 1.968602825745683e-05,
      "loss": 0.4652,
      "step": 700
    },
    {
      "epoch": 0.2681268882175227,
      "grad_norm": 5.7516584396362305,
      "learning_rate": 1.967032967032967e-05,
      "loss": 0.4517,
      "step": 710
    },
    {
      "epoch": 0.2719033232628399,
      "grad_norm": 7.812452793121338,
      "learning_rate": 1.9654631083202513e-05,
      "loss": 0.5172,
      "step": 720
    },
    {
      "epoch": 0.2756797583081571,
      "grad_norm": 5.049378871917725,
      "learning_rate": 1.9638932496075356e-05,
      "loss": 0.5287,
      "step": 730
    },
    {
      "epoch": 0.2794561933534743,
      "grad_norm": 9.489540100097656,
      "learning_rate": 1.9623233908948195e-05,
      "loss": 0.4173,
      "step": 740
    },
    {
      "epoch": 0.28323262839879154,
      "grad_norm": 8.9802885055542,
      "learning_rate": 1.9607535321821038e-05,
      "loss": 0.4634,
      "step": 750
    },
    {
      "epoch": 0.28700906344410876,
      "grad_norm": 9.369575500488281,
      "learning_rate": 1.9591836734693877e-05,
      "loss": 0.381,
      "step": 760
    },
    {
      "epoch": 0.290785498489426,
      "grad_norm": 6.04770565032959,
      "learning_rate": 1.957613814756672e-05,
      "loss": 0.5201,
      "step": 770
    },
    {
      "epoch": 0.2945619335347432,
      "grad_norm": 7.625728607177734,
      "learning_rate": 1.9560439560439563e-05,
      "loss": 0.4729,
      "step": 780
    },
    {
      "epoch": 0.2983383685800604,
      "grad_norm": 10.220743179321289,
      "learning_rate": 1.9544740973312402e-05,
      "loss": 0.4917,
      "step": 790
    },
    {
      "epoch": 0.3021148036253776,
      "grad_norm": 6.457905292510986,
      "learning_rate": 1.9529042386185245e-05,
      "loss": 0.4711,
      "step": 800
    },
    {
      "epoch": 0.30589123867069484,
      "grad_norm": 9.17966365814209,
      "learning_rate": 1.9513343799058088e-05,
      "loss": 0.4612,
      "step": 810
    },
    {
      "epoch": 0.30966767371601206,
      "grad_norm": 5.949531555175781,
      "learning_rate": 1.9497645211930927e-05,
      "loss": 0.4845,
      "step": 820
    },
    {
      "epoch": 0.31344410876132933,
      "grad_norm": 6.4293599128723145,
      "learning_rate": 1.948194662480377e-05,
      "loss": 0.4809,
      "step": 830
    },
    {
      "epoch": 0.31722054380664655,
      "grad_norm": 8.170917510986328,
      "learning_rate": 1.9466248037676613e-05,
      "loss": 0.4563,
      "step": 840
    },
    {
      "epoch": 0.32099697885196377,
      "grad_norm": 3.2692971229553223,
      "learning_rate": 1.9450549450549452e-05,
      "loss": 0.4103,
      "step": 850
    },
    {
      "epoch": 0.324773413897281,
      "grad_norm": 8.552648544311523,
      "learning_rate": 1.9434850863422295e-05,
      "loss": 0.4491,
      "step": 860
    },
    {
      "epoch": 0.3285498489425982,
      "grad_norm": 4.43472146987915,
      "learning_rate": 1.9419152276295135e-05,
      "loss": 0.4094,
      "step": 870
    },
    {
      "epoch": 0.3323262839879154,
      "grad_norm": 5.380845069885254,
      "learning_rate": 1.9403453689167977e-05,
      "loss": 0.4317,
      "step": 880
    },
    {
      "epoch": 0.33610271903323263,
      "grad_norm": 6.587189674377441,
      "learning_rate": 1.9387755102040817e-05,
      "loss": 0.4949,
      "step": 890
    },
    {
      "epoch": 0.33987915407854985,
      "grad_norm": 8.307133674621582,
      "learning_rate": 1.937205651491366e-05,
      "loss": 0.5014,
      "step": 900
    },
    {
      "epoch": 0.34365558912386707,
      "grad_norm": 8.290437698364258,
      "learning_rate": 1.93563579277865e-05,
      "loss": 0.4282,
      "step": 910
    },
    {
      "epoch": 0.3474320241691843,
      "grad_norm": 7.952579498291016,
      "learning_rate": 1.9340659340659342e-05,
      "loss": 0.5146,
      "step": 920
    },
    {
      "epoch": 0.3512084592145015,
      "grad_norm": 5.771968364715576,
      "learning_rate": 1.932496075353218e-05,
      "loss": 0.4658,
      "step": 930
    },
    {
      "epoch": 0.3549848942598187,
      "grad_norm": 5.640376567840576,
      "learning_rate": 1.9309262166405024e-05,
      "loss": 0.4104,
      "step": 940
    },
    {
      "epoch": 0.35876132930513593,
      "grad_norm": 5.0853657722473145,
      "learning_rate": 1.9293563579277867e-05,
      "loss": 0.4386,
      "step": 950
    },
    {
      "epoch": 0.36253776435045315,
      "grad_norm": 7.9172587394714355,
      "learning_rate": 1.9277864992150706e-05,
      "loss": 0.5204,
      "step": 960
    },
    {
      "epoch": 0.36631419939577037,
      "grad_norm": 5.034547328948975,
      "learning_rate": 1.926216640502355e-05,
      "loss": 0.3982,
      "step": 970
    },
    {
      "epoch": 0.37009063444108764,
      "grad_norm": 3.836503028869629,
      "learning_rate": 1.9246467817896392e-05,
      "loss": 0.4393,
      "step": 980
    },
    {
      "epoch": 0.37386706948640486,
      "grad_norm": 6.140079498291016,
      "learning_rate": 1.923076923076923e-05,
      "loss": 0.3445,
      "step": 990
    },
    {
      "epoch": 0.3776435045317221,
      "grad_norm": 3.003798007965088,
      "learning_rate": 1.9215070643642074e-05,
      "loss": 0.4275,
      "step": 1000
    },
    {
      "epoch": 0.3814199395770393,
      "grad_norm": 6.8019609451293945,
      "learning_rate": 1.9199372056514917e-05,
      "loss": 0.4631,
      "step": 1010
    },
    {
      "epoch": 0.3851963746223565,
      "grad_norm": 4.644720077514648,
      "learning_rate": 1.9183673469387756e-05,
      "loss": 0.5209,
      "step": 1020
    },
    {
      "epoch": 0.3889728096676737,
      "grad_norm": 6.723182678222656,
      "learning_rate": 1.91679748822606e-05,
      "loss": 0.3917,
      "step": 1030
    },
    {
      "epoch": 0.39274924471299094,
      "grad_norm": 4.6732940673828125,
      "learning_rate": 1.9152276295133442e-05,
      "loss": 0.3911,
      "step": 1040
    },
    {
      "epoch": 0.39652567975830816,
      "grad_norm": 8.482345581054688,
      "learning_rate": 1.913657770800628e-05,
      "loss": 0.3431,
      "step": 1050
    },
    {
      "epoch": 0.4003021148036254,
      "grad_norm": 7.5789313316345215,
      "learning_rate": 1.9120879120879124e-05,
      "loss": 0.4487,
      "step": 1060
    },
    {
      "epoch": 0.4040785498489426,
      "grad_norm": 7.122939109802246,
      "learning_rate": 1.9105180533751964e-05,
      "loss": 0.423,
      "step": 1070
    },
    {
      "epoch": 0.4078549848942598,
      "grad_norm": 8.539225578308105,
      "learning_rate": 1.9089481946624807e-05,
      "loss": 0.4726,
      "step": 1080
    },
    {
      "epoch": 0.411631419939577,
      "grad_norm": 12.187615394592285,
      "learning_rate": 1.9073783359497646e-05,
      "loss": 0.4361,
      "step": 1090
    },
    {
      "epoch": 0.41540785498489424,
      "grad_norm": 5.518585681915283,
      "learning_rate": 1.905808477237049e-05,
      "loss": 0.3897,
      "step": 1100
    },
    {
      "epoch": 0.41918429003021146,
      "grad_norm": 7.6297993659973145,
      "learning_rate": 1.9042386185243328e-05,
      "loss": 0.5046,
      "step": 1110
    },
    {
      "epoch": 0.4229607250755287,
      "grad_norm": 7.0093841552734375,
      "learning_rate": 1.902668759811617e-05,
      "loss": 0.4128,
      "step": 1120
    },
    {
      "epoch": 0.42673716012084595,
      "grad_norm": 8.397564888000488,
      "learning_rate": 1.901098901098901e-05,
      "loss": 0.3875,
      "step": 1130
    },
    {
      "epoch": 0.43051359516616317,
      "grad_norm": 6.0513105392456055,
      "learning_rate": 1.8995290423861853e-05,
      "loss": 0.4375,
      "step": 1140
    },
    {
      "epoch": 0.4342900302114804,
      "grad_norm": 7.473301410675049,
      "learning_rate": 1.8979591836734696e-05,
      "loss": 0.472,
      "step": 1150
    },
    {
      "epoch": 0.4380664652567976,
      "grad_norm": 4.73439359664917,
      "learning_rate": 1.8963893249607535e-05,
      "loss": 0.4238,
      "step": 1160
    },
    {
      "epoch": 0.4418429003021148,
      "grad_norm": 10.96198844909668,
      "learning_rate": 1.8948194662480378e-05,
      "loss": 0.4126,
      "step": 1170
    },
    {
      "epoch": 0.44561933534743203,
      "grad_norm": 16.162275314331055,
      "learning_rate": 1.893249607535322e-05,
      "loss": 0.5316,
      "step": 1180
    },
    {
      "epoch": 0.44939577039274925,
      "grad_norm": 6.148488521575928,
      "learning_rate": 1.891679748822606e-05,
      "loss": 0.3897,
      "step": 1190
    },
    {
      "epoch": 0.45317220543806647,
      "grad_norm": 5.657165050506592,
      "learning_rate": 1.8901098901098903e-05,
      "loss": 0.5154,
      "step": 1200
    },
    {
      "epoch": 0.4569486404833837,
      "grad_norm": 4.735080242156982,
      "learning_rate": 1.8885400313971746e-05,
      "loss": 0.4219,
      "step": 1210
    },
    {
      "epoch": 0.4607250755287009,
      "grad_norm": 5.873309135437012,
      "learning_rate": 1.8869701726844586e-05,
      "loss": 0.4358,
      "step": 1220
    },
    {
      "epoch": 0.4645015105740181,
      "grad_norm": 5.322498321533203,
      "learning_rate": 1.885400313971743e-05,
      "loss": 0.4519,
      "step": 1230
    },
    {
      "epoch": 0.46827794561933533,
      "grad_norm": 2.6274971961975098,
      "learning_rate": 1.8838304552590268e-05,
      "loss": 0.4069,
      "step": 1240
    },
    {
      "epoch": 0.47205438066465255,
      "grad_norm": 6.133754253387451,
      "learning_rate": 1.882260596546311e-05,
      "loss": 0.3716,
      "step": 1250
    },
    {
      "epoch": 0.47583081570996977,
      "grad_norm": 8.262606620788574,
      "learning_rate": 1.8806907378335953e-05,
      "loss": 0.4359,
      "step": 1260
    },
    {
      "epoch": 0.479607250755287,
      "grad_norm": 6.833771228790283,
      "learning_rate": 1.8791208791208793e-05,
      "loss": 0.4131,
      "step": 1270
    },
    {
      "epoch": 0.48338368580060426,
      "grad_norm": 5.217029094696045,
      "learning_rate": 1.8775510204081636e-05,
      "loss": 0.5386,
      "step": 1280
    },
    {
      "epoch": 0.4871601208459215,
      "grad_norm": 5.557926654815674,
      "learning_rate": 1.8759811616954475e-05,
      "loss": 0.4731,
      "step": 1290
    },
    {
      "epoch": 0.4909365558912387,
      "grad_norm": 3.3124840259552,
      "learning_rate": 1.8744113029827318e-05,
      "loss": 0.4172,
      "step": 1300
    },
    {
      "epoch": 0.4947129909365559,
      "grad_norm": 5.529379844665527,
      "learning_rate": 1.8728414442700157e-05,
      "loss": 0.551,
      "step": 1310
    },
    {
      "epoch": 0.4984894259818731,
      "grad_norm": 4.310783863067627,
      "learning_rate": 1.8712715855573e-05,
      "loss": 0.3738,
      "step": 1320
    },
    {
      "epoch": 0.5022658610271903,
      "grad_norm": 5.537252426147461,
      "learning_rate": 1.869701726844584e-05,
      "loss": 0.3889,
      "step": 1330
    },
    {
      "epoch": 0.5060422960725075,
      "grad_norm": 13.09440803527832,
      "learning_rate": 1.8681318681318682e-05,
      "loss": 0.4893,
      "step": 1340
    },
    {
      "epoch": 0.5098187311178247,
      "grad_norm": 3.8895537853240967,
      "learning_rate": 1.866562009419152e-05,
      "loss": 0.4712,
      "step": 1350
    },
    {
      "epoch": 0.513595166163142,
      "grad_norm": 4.015820026397705,
      "learning_rate": 1.8649921507064364e-05,
      "loss": 0.4567,
      "step": 1360
    },
    {
      "epoch": 0.5173716012084593,
      "grad_norm": 12.622395515441895,
      "learning_rate": 1.8634222919937207e-05,
      "loss": 0.3568,
      "step": 1370
    },
    {
      "epoch": 0.5211480362537765,
      "grad_norm": 8.202075004577637,
      "learning_rate": 1.861852433281005e-05,
      "loss": 0.4306,
      "step": 1380
    },
    {
      "epoch": 0.5249244712990937,
      "grad_norm": 6.650628089904785,
      "learning_rate": 1.860282574568289e-05,
      "loss": 0.5066,
      "step": 1390
    },
    {
      "epoch": 0.5287009063444109,
      "grad_norm": 4.815984725952148,
      "learning_rate": 1.8587127158555732e-05,
      "loss": 0.368,
      "step": 1400
    },
    {
      "epoch": 0.5324773413897281,
      "grad_norm": 3.0683398246765137,
      "learning_rate": 1.8571428571428575e-05,
      "loss": 0.3853,
      "step": 1410
    },
    {
      "epoch": 0.5362537764350453,
      "grad_norm": 6.164922714233398,
      "learning_rate": 1.8555729984301415e-05,
      "loss": 0.3996,
      "step": 1420
    },
    {
      "epoch": 0.5400302114803626,
      "grad_norm": 8.204878807067871,
      "learning_rate": 1.8540031397174257e-05,
      "loss": 0.4669,
      "step": 1430
    },
    {
      "epoch": 0.5438066465256798,
      "grad_norm": 9.021702766418457,
      "learning_rate": 1.8524332810047097e-05,
      "loss": 0.465,
      "step": 1440
    },
    {
      "epoch": 0.547583081570997,
      "grad_norm": 5.1211771965026855,
      "learning_rate": 1.850863422291994e-05,
      "loss": 0.406,
      "step": 1450
    },
    {
      "epoch": 0.5513595166163142,
      "grad_norm": 4.642108917236328,
      "learning_rate": 1.849293563579278e-05,
      "loss": 0.3735,
      "step": 1460
    },
    {
      "epoch": 0.5551359516616314,
      "grad_norm": 7.058592796325684,
      "learning_rate": 1.8477237048665622e-05,
      "loss": 0.6226,
      "step": 1470
    },
    {
      "epoch": 0.5589123867069486,
      "grad_norm": 4.513877868652344,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 0.4463,
      "step": 1480
    },
    {
      "epoch": 0.5626888217522659,
      "grad_norm": 6.1806230545043945,
      "learning_rate": 1.8445839874411304e-05,
      "loss": 0.4378,
      "step": 1490
    },
    {
      "epoch": 0.5664652567975831,
      "grad_norm": 5.61577033996582,
      "learning_rate": 1.8430141287284147e-05,
      "loss": 0.4059,
      "step": 1500
    },
    {
      "epoch": 0.5702416918429003,
      "grad_norm": 3.2693653106689453,
      "learning_rate": 1.8414442700156986e-05,
      "loss": 0.3792,
      "step": 1510
    },
    {
      "epoch": 0.5740181268882175,
      "grad_norm": 9.245620727539062,
      "learning_rate": 1.839874411302983e-05,
      "loss": 0.4369,
      "step": 1520
    },
    {
      "epoch": 0.5777945619335347,
      "grad_norm": 10.159503936767578,
      "learning_rate": 1.838304552590267e-05,
      "loss": 0.381,
      "step": 1530
    },
    {
      "epoch": 0.581570996978852,
      "grad_norm": 4.687349796295166,
      "learning_rate": 1.836734693877551e-05,
      "loss": 0.404,
      "step": 1540
    },
    {
      "epoch": 0.5853474320241692,
      "grad_norm": 6.419262409210205,
      "learning_rate": 1.835164835164835e-05,
      "loss": 0.4694,
      "step": 1550
    },
    {
      "epoch": 0.5891238670694864,
      "grad_norm": 5.427120208740234,
      "learning_rate": 1.8335949764521194e-05,
      "loss": 0.3439,
      "step": 1560
    },
    {
      "epoch": 0.5929003021148036,
      "grad_norm": 11.649142265319824,
      "learning_rate": 1.8320251177394036e-05,
      "loss": 0.411,
      "step": 1570
    },
    {
      "epoch": 0.5966767371601208,
      "grad_norm": 9.368894577026367,
      "learning_rate": 1.830455259026688e-05,
      "loss": 0.45,
      "step": 1580
    },
    {
      "epoch": 0.600453172205438,
      "grad_norm": 6.626433849334717,
      "learning_rate": 1.828885400313972e-05,
      "loss": 0.3961,
      "step": 1590
    },
    {
      "epoch": 0.6042296072507553,
      "grad_norm": 4.613400936126709,
      "learning_rate": 1.827315541601256e-05,
      "loss": 0.3906,
      "step": 1600
    },
    {
      "epoch": 0.6080060422960725,
      "grad_norm": 2.868727684020996,
      "learning_rate": 1.8257456828885404e-05,
      "loss": 0.3698,
      "step": 1610
    },
    {
      "epoch": 0.6117824773413897,
      "grad_norm": 10.74678897857666,
      "learning_rate": 1.8241758241758244e-05,
      "loss": 0.5334,
      "step": 1620
    },
    {
      "epoch": 0.6155589123867069,
      "grad_norm": 4.456183910369873,
      "learning_rate": 1.8226059654631086e-05,
      "loss": 0.4691,
      "step": 1630
    },
    {
      "epoch": 0.6193353474320241,
      "grad_norm": 4.598874092102051,
      "learning_rate": 1.8210361067503926e-05,
      "loss": 0.4656,
      "step": 1640
    },
    {
      "epoch": 0.6231117824773413,
      "grad_norm": 4.9702324867248535,
      "learning_rate": 1.819466248037677e-05,
      "loss": 0.4239,
      "step": 1650
    },
    {
      "epoch": 0.6268882175226587,
      "grad_norm": 4.533648490905762,
      "learning_rate": 1.8178963893249608e-05,
      "loss": 0.4128,
      "step": 1660
    },
    {
      "epoch": 0.6306646525679759,
      "grad_norm": 6.496919631958008,
      "learning_rate": 1.816326530612245e-05,
      "loss": 0.4038,
      "step": 1670
    },
    {
      "epoch": 0.6344410876132931,
      "grad_norm": 5.903757572174072,
      "learning_rate": 1.814756671899529e-05,
      "loss": 0.4745,
      "step": 1680
    },
    {
      "epoch": 0.6382175226586103,
      "grad_norm": 4.145135879516602,
      "learning_rate": 1.8131868131868133e-05,
      "loss": 0.3988,
      "step": 1690
    },
    {
      "epoch": 0.6419939577039275,
      "grad_norm": 8.344403266906738,
      "learning_rate": 1.8116169544740976e-05,
      "loss": 0.4376,
      "step": 1700
    },
    {
      "epoch": 0.6457703927492447,
      "grad_norm": 5.0644001960754395,
      "learning_rate": 1.8100470957613815e-05,
      "loss": 0.3056,
      "step": 1710
    },
    {
      "epoch": 0.649546827794562,
      "grad_norm": 5.9599223136901855,
      "learning_rate": 1.8084772370486658e-05,
      "loss": 0.4521,
      "step": 1720
    },
    {
      "epoch": 0.6533232628398792,
      "grad_norm": 5.165347099304199,
      "learning_rate": 1.8069073783359498e-05,
      "loss": 0.5132,
      "step": 1730
    },
    {
      "epoch": 0.6570996978851964,
      "grad_norm": 4.8693623542785645,
      "learning_rate": 1.805337519623234e-05,
      "loss": 0.4099,
      "step": 1740
    },
    {
      "epoch": 0.6608761329305136,
      "grad_norm": 9.34113883972168,
      "learning_rate": 1.803767660910518e-05,
      "loss": 0.396,
      "step": 1750
    },
    {
      "epoch": 0.6646525679758308,
      "grad_norm": 8.471875190734863,
      "learning_rate": 1.8021978021978023e-05,
      "loss": 0.4608,
      "step": 1760
    },
    {
      "epoch": 0.668429003021148,
      "grad_norm": 4.227803707122803,
      "learning_rate": 1.8006279434850865e-05,
      "loss": 0.4212,
      "step": 1770
    },
    {
      "epoch": 0.6722054380664653,
      "grad_norm": 7.425074100494385,
      "learning_rate": 1.7990580847723708e-05,
      "loss": 0.378,
      "step": 1780
    },
    {
      "epoch": 0.6759818731117825,
      "grad_norm": 7.517932891845703,
      "learning_rate": 1.7974882260596548e-05,
      "loss": 0.331,
      "step": 1790
    },
    {
      "epoch": 0.6797583081570997,
      "grad_norm": 7.532932758331299,
      "learning_rate": 1.795918367346939e-05,
      "loss": 0.3793,
      "step": 1800
    },
    {
      "epoch": 0.6835347432024169,
      "grad_norm": 4.667819976806641,
      "learning_rate": 1.7943485086342233e-05,
      "loss": 0.4442,
      "step": 1810
    },
    {
      "epoch": 0.6873111782477341,
      "grad_norm": 3.575746536254883,
      "learning_rate": 1.7927786499215073e-05,
      "loss": 0.4221,
      "step": 1820
    },
    {
      "epoch": 0.6910876132930514,
      "grad_norm": 6.650301933288574,
      "learning_rate": 1.7912087912087915e-05,
      "loss": 0.3628,
      "step": 1830
    },
    {
      "epoch": 0.6948640483383686,
      "grad_norm": 4.321619987487793,
      "learning_rate": 1.7896389324960755e-05,
      "loss": 0.3855,
      "step": 1840
    },
    {
      "epoch": 0.6986404833836858,
      "grad_norm": 6.50283670425415,
      "learning_rate": 1.7880690737833598e-05,
      "loss": 0.3571,
      "step": 1850
    },
    {
      "epoch": 0.702416918429003,
      "grad_norm": 7.071866512298584,
      "learning_rate": 1.7864992150706437e-05,
      "loss": 0.4795,
      "step": 1860
    },
    {
      "epoch": 0.7061933534743202,
      "grad_norm": 6.054981231689453,
      "learning_rate": 1.784929356357928e-05,
      "loss": 0.3887,
      "step": 1870
    },
    {
      "epoch": 0.7099697885196374,
      "grad_norm": 6.3962016105651855,
      "learning_rate": 1.783359497645212e-05,
      "loss": 0.3745,
      "step": 1880
    },
    {
      "epoch": 0.7137462235649547,
      "grad_norm": 8.59218692779541,
      "learning_rate": 1.7817896389324962e-05,
      "loss": 0.419,
      "step": 1890
    },
    {
      "epoch": 0.7175226586102719,
      "grad_norm": 6.009099960327148,
      "learning_rate": 1.78021978021978e-05,
      "loss": 0.373,
      "step": 1900
    },
    {
      "epoch": 0.7212990936555891,
      "grad_norm": 6.125227451324463,
      "learning_rate": 1.7786499215070644e-05,
      "loss": 0.3919,
      "step": 1910
    },
    {
      "epoch": 0.7250755287009063,
      "grad_norm": 3.6593589782714844,
      "learning_rate": 1.7770800627943487e-05,
      "loss": 0.4122,
      "step": 1920
    },
    {
      "epoch": 0.7288519637462235,
      "grad_norm": 8.018628120422363,
      "learning_rate": 1.7755102040816327e-05,
      "loss": 0.4866,
      "step": 1930
    },
    {
      "epoch": 0.7326283987915407,
      "grad_norm": 14.10633373260498,
      "learning_rate": 1.773940345368917e-05,
      "loss": 0.396,
      "step": 1940
    },
    {
      "epoch": 0.736404833836858,
      "grad_norm": 4.50093936920166,
      "learning_rate": 1.772370486656201e-05,
      "loss": 0.365,
      "step": 1950
    },
    {
      "epoch": 0.7401812688821753,
      "grad_norm": 2.7214367389678955,
      "learning_rate": 1.770800627943485e-05,
      "loss": 0.3829,
      "step": 1960
    },
    {
      "epoch": 0.7439577039274925,
      "grad_norm": 16.08682632446289,
      "learning_rate": 1.7692307692307694e-05,
      "loss": 0.4229,
      "step": 1970
    },
    {
      "epoch": 0.7477341389728097,
      "grad_norm": 3.6087253093719482,
      "learning_rate": 1.7676609105180537e-05,
      "loss": 0.48,
      "step": 1980
    },
    {
      "epoch": 0.7515105740181269,
      "grad_norm": 3.7809574604034424,
      "learning_rate": 1.7660910518053377e-05,
      "loss": 0.3986,
      "step": 1990
    },
    {
      "epoch": 0.7552870090634441,
      "grad_norm": 7.323451995849609,
      "learning_rate": 1.764521193092622e-05,
      "loss": 0.4022,
      "step": 2000
    },
    {
      "epoch": 0.7590634441087614,
      "grad_norm": 6.005460739135742,
      "learning_rate": 1.762951334379906e-05,
      "loss": 0.5028,
      "step": 2010
    },
    {
      "epoch": 0.7628398791540786,
      "grad_norm": 7.126471042633057,
      "learning_rate": 1.76138147566719e-05,
      "loss": 0.4574,
      "step": 2020
    },
    {
      "epoch": 0.7666163141993958,
      "grad_norm": 4.335982322692871,
      "learning_rate": 1.7598116169544744e-05,
      "loss": 0.4343,
      "step": 2030
    },
    {
      "epoch": 0.770392749244713,
      "grad_norm": 9.332865715026855,
      "learning_rate": 1.7582417582417584e-05,
      "loss": 0.2697,
      "step": 2040
    },
    {
      "epoch": 0.7741691842900302,
      "grad_norm": 4.597686290740967,
      "learning_rate": 1.7566718995290427e-05,
      "loss": 0.4001,
      "step": 2050
    },
    {
      "epoch": 0.7779456193353474,
      "grad_norm": 12.721891403198242,
      "learning_rate": 1.7551020408163266e-05,
      "loss": 0.44,
      "step": 2060
    },
    {
      "epoch": 0.7817220543806647,
      "grad_norm": 5.615993499755859,
      "learning_rate": 1.753532182103611e-05,
      "loss": 0.431,
      "step": 2070
    },
    {
      "epoch": 0.7854984894259819,
      "grad_norm": 4.426461219787598,
      "learning_rate": 1.751962323390895e-05,
      "loss": 0.4928,
      "step": 2080
    },
    {
      "epoch": 0.7892749244712991,
      "grad_norm": 3.2694740295410156,
      "learning_rate": 1.750392464678179e-05,
      "loss": 0.4809,
      "step": 2090
    },
    {
      "epoch": 0.7930513595166163,
      "grad_norm": 3.8588364124298096,
      "learning_rate": 1.748822605965463e-05,
      "loss": 0.378,
      "step": 2100
    },
    {
      "epoch": 0.7968277945619335,
      "grad_norm": 3.6186912059783936,
      "learning_rate": 1.7472527472527473e-05,
      "loss": 0.4387,
      "step": 2110
    },
    {
      "epoch": 0.8006042296072508,
      "grad_norm": 6.8597235679626465,
      "learning_rate": 1.7456828885400316e-05,
      "loss": 0.3772,
      "step": 2120
    },
    {
      "epoch": 0.804380664652568,
      "grad_norm": 3.178219795227051,
      "learning_rate": 1.7441130298273156e-05,
      "loss": 0.4058,
      "step": 2130
    },
    {
      "epoch": 0.8081570996978852,
      "grad_norm": 10.191954612731934,
      "learning_rate": 1.7425431711146e-05,
      "loss": 0.4443,
      "step": 2140
    },
    {
      "epoch": 0.8119335347432024,
      "grad_norm": 6.737580299377441,
      "learning_rate": 1.7409733124018838e-05,
      "loss": 0.4059,
      "step": 2150
    },
    {
      "epoch": 0.8157099697885196,
      "grad_norm": 5.5456318855285645,
      "learning_rate": 1.739403453689168e-05,
      "loss": 0.3638,
      "step": 2160
    },
    {
      "epoch": 0.8194864048338368,
      "grad_norm": 3.4932096004486084,
      "learning_rate": 1.7378335949764523e-05,
      "loss": 0.3891,
      "step": 2170
    },
    {
      "epoch": 0.823262839879154,
      "grad_norm": 4.579352378845215,
      "learning_rate": 1.7362637362637363e-05,
      "loss": 0.4208,
      "step": 2180
    },
    {
      "epoch": 0.8270392749244713,
      "grad_norm": 4.904726982116699,
      "learning_rate": 1.7346938775510206e-05,
      "loss": 0.3348,
      "step": 2190
    },
    {
      "epoch": 0.8308157099697885,
      "grad_norm": 7.1527533531188965,
      "learning_rate": 1.733124018838305e-05,
      "loss": 0.3937,
      "step": 2200
    },
    {
      "epoch": 0.8345921450151057,
      "grad_norm": 5.6854963302612305,
      "learning_rate": 1.7315541601255888e-05,
      "loss": 0.4017,
      "step": 2210
    },
    {
      "epoch": 0.8383685800604229,
      "grad_norm": 5.420626640319824,
      "learning_rate": 1.729984301412873e-05,
      "loss": 0.4298,
      "step": 2220
    },
    {
      "epoch": 0.8421450151057401,
      "grad_norm": 6.683321475982666,
      "learning_rate": 1.7284144427001574e-05,
      "loss": 0.4548,
      "step": 2230
    },
    {
      "epoch": 0.8459214501510574,
      "grad_norm": 5.247477054595947,
      "learning_rate": 1.7268445839874413e-05,
      "loss": 0.4078,
      "step": 2240
    },
    {
      "epoch": 0.8496978851963746,
      "grad_norm": 6.630544185638428,
      "learning_rate": 1.7252747252747256e-05,
      "loss": 0.4791,
      "step": 2250
    },
    {
      "epoch": 0.8534743202416919,
      "grad_norm": 5.22444486618042,
      "learning_rate": 1.7237048665620095e-05,
      "loss": 0.4247,
      "step": 2260
    },
    {
      "epoch": 0.8572507552870091,
      "grad_norm": 4.433542251586914,
      "learning_rate": 1.7221350078492938e-05,
      "loss": 0.4115,
      "step": 2270
    },
    {
      "epoch": 0.8610271903323263,
      "grad_norm": 4.815728664398193,
      "learning_rate": 1.7205651491365777e-05,
      "loss": 0.3329,
      "step": 2280
    },
    {
      "epoch": 0.8648036253776435,
      "grad_norm": 6.169913291931152,
      "learning_rate": 1.718995290423862e-05,
      "loss": 0.4304,
      "step": 2290
    },
    {
      "epoch": 0.8685800604229608,
      "grad_norm": 4.404515743255615,
      "learning_rate": 1.717425431711146e-05,
      "loss": 0.4444,
      "step": 2300
    },
    {
      "epoch": 0.872356495468278,
      "grad_norm": 4.984467506408691,
      "learning_rate": 1.7158555729984302e-05,
      "loss": 0.377,
      "step": 2310
    },
    {
      "epoch": 0.8761329305135952,
      "grad_norm": 10.497851371765137,
      "learning_rate": 1.7142857142857142e-05,
      "loss": 0.4378,
      "step": 2320
    },
    {
      "epoch": 0.8799093655589124,
      "grad_norm": 3.7247889041900635,
      "learning_rate": 1.7127158555729985e-05,
      "loss": 0.4034,
      "step": 2330
    },
    {
      "epoch": 0.8836858006042296,
      "grad_norm": 4.361709117889404,
      "learning_rate": 1.7111459968602827e-05,
      "loss": 0.4041,
      "step": 2340
    },
    {
      "epoch": 0.8874622356495468,
      "grad_norm": 7.6900153160095215,
      "learning_rate": 1.7095761381475667e-05,
      "loss": 0.3901,
      "step": 2350
    },
    {
      "epoch": 0.8912386706948641,
      "grad_norm": 11.598121643066406,
      "learning_rate": 1.708006279434851e-05,
      "loss": 0.4786,
      "step": 2360
    },
    {
      "epoch": 0.8950151057401813,
      "grad_norm": 3.945887804031372,
      "learning_rate": 1.7064364207221352e-05,
      "loss": 0.4168,
      "step": 2370
    },
    {
      "epoch": 0.8987915407854985,
      "grad_norm": 9.11853313446045,
      "learning_rate": 1.7048665620094192e-05,
      "loss": 0.3845,
      "step": 2380
    },
    {
      "epoch": 0.9025679758308157,
      "grad_norm": 2.5923266410827637,
      "learning_rate": 1.7032967032967035e-05,
      "loss": 0.4413,
      "step": 2390
    },
    {
      "epoch": 0.9063444108761329,
      "grad_norm": 6.685782432556152,
      "learning_rate": 1.7017268445839878e-05,
      "loss": 0.3611,
      "step": 2400
    },
    {
      "epoch": 0.9101208459214502,
      "grad_norm": 8.421564102172852,
      "learning_rate": 1.7001569858712717e-05,
      "loss": 0.4192,
      "step": 2410
    },
    {
      "epoch": 0.9138972809667674,
      "grad_norm": 4.175644397735596,
      "learning_rate": 1.698587127158556e-05,
      "loss": 0.4386,
      "step": 2420
    },
    {
      "epoch": 0.9176737160120846,
      "grad_norm": 3.8832616806030273,
      "learning_rate": 1.69701726844584e-05,
      "loss": 0.4002,
      "step": 2430
    },
    {
      "epoch": 0.9214501510574018,
      "grad_norm": 22.96501350402832,
      "learning_rate": 1.6954474097331242e-05,
      "loss": 0.4416,
      "step": 2440
    },
    {
      "epoch": 0.925226586102719,
      "grad_norm": 6.108251571655273,
      "learning_rate": 1.6938775510204085e-05,
      "loss": 0.4145,
      "step": 2450
    },
    {
      "epoch": 0.9290030211480362,
      "grad_norm": 4.2986626625061035,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 0.3564,
      "step": 2460
    },
    {
      "epoch": 0.9327794561933535,
      "grad_norm": 2.649122953414917,
      "learning_rate": 1.6907378335949767e-05,
      "loss": 0.378,
      "step": 2470
    },
    {
      "epoch": 0.9365558912386707,
      "grad_norm": 7.539280891418457,
      "learning_rate": 1.6891679748822606e-05,
      "loss": 0.4048,
      "step": 2480
    },
    {
      "epoch": 0.9403323262839879,
      "grad_norm": 3.9752438068389893,
      "learning_rate": 1.687598116169545e-05,
      "loss": 0.3543,
      "step": 2490
    },
    {
      "epoch": 0.9441087613293051,
      "grad_norm": 4.2723259925842285,
      "learning_rate": 1.686028257456829e-05,
      "loss": 0.3078,
      "step": 2500
    },
    {
      "epoch": 0.9478851963746223,
      "grad_norm": 9.313314437866211,
      "learning_rate": 1.684458398744113e-05,
      "loss": 0.3238,
      "step": 2510
    },
    {
      "epoch": 0.9516616314199395,
      "grad_norm": 8.327607154846191,
      "learning_rate": 1.682888540031397e-05,
      "loss": 0.4942,
      "step": 2520
    },
    {
      "epoch": 0.9554380664652568,
      "grad_norm": 4.1073899269104,
      "learning_rate": 1.6813186813186814e-05,
      "loss": 0.3802,
      "step": 2530
    },
    {
      "epoch": 0.959214501510574,
      "grad_norm": 6.8091630935668945,
      "learning_rate": 1.6797488226059656e-05,
      "loss": 0.3843,
      "step": 2540
    },
    {
      "epoch": 0.9629909365558912,
      "grad_norm": 4.909342288970947,
      "learning_rate": 1.6781789638932496e-05,
      "loss": 0.3669,
      "step": 2550
    },
    {
      "epoch": 0.9667673716012085,
      "grad_norm": 5.9022932052612305,
      "learning_rate": 1.676609105180534e-05,
      "loss": 0.4875,
      "step": 2560
    },
    {
      "epoch": 0.9705438066465257,
      "grad_norm": 3.3981080055236816,
      "learning_rate": 1.675039246467818e-05,
      "loss": 0.4258,
      "step": 2570
    },
    {
      "epoch": 0.974320241691843,
      "grad_norm": 4.292660713195801,
      "learning_rate": 1.673469387755102e-05,
      "loss": 0.4227,
      "step": 2580
    },
    {
      "epoch": 0.9780966767371602,
      "grad_norm": 3.325716018676758,
      "learning_rate": 1.6718995290423864e-05,
      "loss": 0.3039,
      "step": 2590
    },
    {
      "epoch": 0.9818731117824774,
      "grad_norm": 8.128891944885254,
      "learning_rate": 1.6703296703296707e-05,
      "loss": 0.3575,
      "step": 2600
    },
    {
      "epoch": 0.9856495468277946,
      "grad_norm": 4.658071517944336,
      "learning_rate": 1.6687598116169546e-05,
      "loss": 0.3912,
      "step": 2610
    },
    {
      "epoch": 0.9894259818731118,
      "grad_norm": 8.422600746154785,
      "learning_rate": 1.667189952904239e-05,
      "loss": 0.4627,
      "step": 2620
    },
    {
      "epoch": 0.993202416918429,
      "grad_norm": 5.8998284339904785,
      "learning_rate": 1.6656200941915228e-05,
      "loss": 0.4046,
      "step": 2630
    },
    {
      "epoch": 0.9969788519637462,
      "grad_norm": 3.4957990646362305,
      "learning_rate": 1.664050235478807e-05,
      "loss": 0.3178,
      "step": 2640
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8282179620360751,
      "eval_f1": 0.7916618943992669,
      "eval_loss": 0.4096648395061493,
      "eval_precision": 0.7850976828714221,
      "eval_recall": 0.7983367983367984,
      "eval_runtime": 45.2391,
      "eval_samples_per_second": 234.067,
      "eval_steps_per_second": 3.669,
      "step": 2648
    },
    {
      "epoch": 1.0007552870090635,
      "grad_norm": 9.695185661315918,
      "learning_rate": 1.662480376766091e-05,
      "loss": 0.4577,
      "step": 2650
    },
    {
      "epoch": 1.0045317220543806,
      "grad_norm": 6.256860733032227,
      "learning_rate": 1.6609105180533753e-05,
      "loss": 0.2514,
      "step": 2660
    },
    {
      "epoch": 1.008308157099698,
      "grad_norm": 11.590529441833496,
      "learning_rate": 1.6593406593406596e-05,
      "loss": 0.3294,
      "step": 2670
    },
    {
      "epoch": 1.012084592145015,
      "grad_norm": 7.126543045043945,
      "learning_rate": 1.6577708006279435e-05,
      "loss": 0.3357,
      "step": 2680
    },
    {
      "epoch": 1.0158610271903323,
      "grad_norm": 5.4143829345703125,
      "learning_rate": 1.6562009419152278e-05,
      "loss": 0.2965,
      "step": 2690
    },
    {
      "epoch": 1.0196374622356494,
      "grad_norm": 9.259264945983887,
      "learning_rate": 1.6546310832025118e-05,
      "loss": 0.3511,
      "step": 2700
    },
    {
      "epoch": 1.0234138972809668,
      "grad_norm": 4.6773200035095215,
      "learning_rate": 1.653061224489796e-05,
      "loss": 0.4328,
      "step": 2710
    },
    {
      "epoch": 1.027190332326284,
      "grad_norm": 5.526657581329346,
      "learning_rate": 1.65149136577708e-05,
      "loss": 0.3215,
      "step": 2720
    },
    {
      "epoch": 1.0309667673716012,
      "grad_norm": 6.414103984832764,
      "learning_rate": 1.6499215070643643e-05,
      "loss": 0.2467,
      "step": 2730
    },
    {
      "epoch": 1.0347432024169185,
      "grad_norm": 4.4317426681518555,
      "learning_rate": 1.6483516483516486e-05,
      "loss": 0.3405,
      "step": 2740
    },
    {
      "epoch": 1.0385196374622356,
      "grad_norm": 15.257112503051758,
      "learning_rate": 1.6467817896389325e-05,
      "loss": 0.3808,
      "step": 2750
    },
    {
      "epoch": 1.042296072507553,
      "grad_norm": 6.663450717926025,
      "learning_rate": 1.6452119309262168e-05,
      "loss": 0.2803,
      "step": 2760
    },
    {
      "epoch": 1.04607250755287,
      "grad_norm": 9.134681701660156,
      "learning_rate": 1.643642072213501e-05,
      "loss": 0.3364,
      "step": 2770
    },
    {
      "epoch": 1.0498489425981874,
      "grad_norm": 12.404873847961426,
      "learning_rate": 1.642072213500785e-05,
      "loss": 0.3853,
      "step": 2780
    },
    {
      "epoch": 1.0536253776435045,
      "grad_norm": 8.70060920715332,
      "learning_rate": 1.6405023547880693e-05,
      "loss": 0.353,
      "step": 2790
    },
    {
      "epoch": 1.0574018126888218,
      "grad_norm": 5.899033069610596,
      "learning_rate": 1.6389324960753536e-05,
      "loss": 0.339,
      "step": 2800
    },
    {
      "epoch": 1.061178247734139,
      "grad_norm": 4.503179550170898,
      "learning_rate": 1.6373626373626375e-05,
      "loss": 0.244,
      "step": 2810
    },
    {
      "epoch": 1.0649546827794563,
      "grad_norm": 8.716926574707031,
      "learning_rate": 1.6357927786499218e-05,
      "loss": 0.3513,
      "step": 2820
    },
    {
      "epoch": 1.0687311178247734,
      "grad_norm": 7.88464879989624,
      "learning_rate": 1.6342229199372057e-05,
      "loss": 0.3267,
      "step": 2830
    },
    {
      "epoch": 1.0725075528700907,
      "grad_norm": 9.773104667663574,
      "learning_rate": 1.63265306122449e-05,
      "loss": 0.3329,
      "step": 2840
    },
    {
      "epoch": 1.0762839879154078,
      "grad_norm": 11.403136253356934,
      "learning_rate": 1.631083202511774e-05,
      "loss": 0.3274,
      "step": 2850
    },
    {
      "epoch": 1.0800604229607251,
      "grad_norm": 6.92056941986084,
      "learning_rate": 1.6295133437990582e-05,
      "loss": 0.4041,
      "step": 2860
    },
    {
      "epoch": 1.0838368580060422,
      "grad_norm": 8.15770435333252,
      "learning_rate": 1.6279434850863422e-05,
      "loss": 0.3587,
      "step": 2870
    },
    {
      "epoch": 1.0876132930513596,
      "grad_norm": 7.562314987182617,
      "learning_rate": 1.6263736263736265e-05,
      "loss": 0.3214,
      "step": 2880
    },
    {
      "epoch": 1.0913897280966767,
      "grad_norm": 4.312256336212158,
      "learning_rate": 1.6248037676609107e-05,
      "loss": 0.4013,
      "step": 2890
    },
    {
      "epoch": 1.095166163141994,
      "grad_norm": 8.854288101196289,
      "learning_rate": 1.6232339089481947e-05,
      "loss": 0.2938,
      "step": 2900
    },
    {
      "epoch": 1.098942598187311,
      "grad_norm": 5.515244483947754,
      "learning_rate": 1.621664050235479e-05,
      "loss": 0.3429,
      "step": 2910
    },
    {
      "epoch": 1.1027190332326284,
      "grad_norm": 7.3464274406433105,
      "learning_rate": 1.620094191522763e-05,
      "loss": 0.3219,
      "step": 2920
    },
    {
      "epoch": 1.1064954682779455,
      "grad_norm": 7.628570556640625,
      "learning_rate": 1.6185243328100472e-05,
      "loss": 0.2517,
      "step": 2930
    },
    {
      "epoch": 1.1102719033232629,
      "grad_norm": 6.582311630249023,
      "learning_rate": 1.6169544740973315e-05,
      "loss": 0.3091,
      "step": 2940
    },
    {
      "epoch": 1.11404833836858,
      "grad_norm": 6.784989833831787,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 0.2698,
      "step": 2950
    },
    {
      "epoch": 1.1178247734138973,
      "grad_norm": 3.803041934967041,
      "learning_rate": 1.6138147566718997e-05,
      "loss": 0.4082,
      "step": 2960
    },
    {
      "epoch": 1.1216012084592144,
      "grad_norm": 12.658268928527832,
      "learning_rate": 1.612244897959184e-05,
      "loss": 0.3691,
      "step": 2970
    },
    {
      "epoch": 1.1253776435045317,
      "grad_norm": 5.60326623916626,
      "learning_rate": 1.610675039246468e-05,
      "loss": 0.379,
      "step": 2980
    },
    {
      "epoch": 1.129154078549849,
      "grad_norm": 3.5604984760284424,
      "learning_rate": 1.6091051805337522e-05,
      "loss": 0.3453,
      "step": 2990
    },
    {
      "epoch": 1.1329305135951662,
      "grad_norm": 8.0608491897583,
      "learning_rate": 1.6075353218210365e-05,
      "loss": 0.3605,
      "step": 3000
    },
    {
      "epoch": 1.1367069486404833,
      "grad_norm": 5.380103588104248,
      "learning_rate": 1.6059654631083204e-05,
      "loss": 0.3895,
      "step": 3010
    },
    {
      "epoch": 1.1404833836858006,
      "grad_norm": 8.814144134521484,
      "learning_rate": 1.6043956043956047e-05,
      "loss": 0.4036,
      "step": 3020
    },
    {
      "epoch": 1.144259818731118,
      "grad_norm": 8.178084373474121,
      "learning_rate": 1.6028257456828886e-05,
      "loss": 0.2491,
      "step": 3030
    },
    {
      "epoch": 1.148036253776435,
      "grad_norm": 8.452398300170898,
      "learning_rate": 1.601255886970173e-05,
      "loss": 0.3549,
      "step": 3040
    },
    {
      "epoch": 1.1518126888217524,
      "grad_norm": 3.734060049057007,
      "learning_rate": 1.599686028257457e-05,
      "loss": 0.3707,
      "step": 3050
    },
    {
      "epoch": 1.1555891238670695,
      "grad_norm": 4.18815803527832,
      "learning_rate": 1.598116169544741e-05,
      "loss": 0.3267,
      "step": 3060
    },
    {
      "epoch": 1.1593655589123868,
      "grad_norm": 3.8223769664764404,
      "learning_rate": 1.596546310832025e-05,
      "loss": 0.2534,
      "step": 3070
    },
    {
      "epoch": 1.163141993957704,
      "grad_norm": 8.690108299255371,
      "learning_rate": 1.5949764521193094e-05,
      "loss": 0.3273,
      "step": 3080
    },
    {
      "epoch": 1.1669184290030212,
      "grad_norm": 2.661083936691284,
      "learning_rate": 1.5934065934065933e-05,
      "loss": 0.3588,
      "step": 3090
    },
    {
      "epoch": 1.1706948640483383,
      "grad_norm": 4.661864757537842,
      "learning_rate": 1.5918367346938776e-05,
      "loss": 0.3089,
      "step": 3100
    },
    {
      "epoch": 1.1744712990936557,
      "grad_norm": 5.4560346603393555,
      "learning_rate": 1.590266875981162e-05,
      "loss": 0.3756,
      "step": 3110
    },
    {
      "epoch": 1.1782477341389728,
      "grad_norm": 5.623371124267578,
      "learning_rate": 1.5886970172684458e-05,
      "loss": 0.3694,
      "step": 3120
    },
    {
      "epoch": 1.18202416918429,
      "grad_norm": 5.0496978759765625,
      "learning_rate": 1.58712715855573e-05,
      "loss": 0.3855,
      "step": 3130
    },
    {
      "epoch": 1.1858006042296072,
      "grad_norm": 8.211899757385254,
      "learning_rate": 1.5855572998430144e-05,
      "loss": 0.3314,
      "step": 3140
    },
    {
      "epoch": 1.1895770392749245,
      "grad_norm": 8.146688461303711,
      "learning_rate": 1.5839874411302983e-05,
      "loss": 0.3691,
      "step": 3150
    },
    {
      "epoch": 1.1933534743202416,
      "grad_norm": 4.2491021156311035,
      "learning_rate": 1.5824175824175826e-05,
      "loss": 0.3062,
      "step": 3160
    },
    {
      "epoch": 1.197129909365559,
      "grad_norm": 2.7399580478668213,
      "learning_rate": 1.580847723704867e-05,
      "loss": 0.2938,
      "step": 3170
    },
    {
      "epoch": 1.200906344410876,
      "grad_norm": 8.259794235229492,
      "learning_rate": 1.5792778649921508e-05,
      "loss": 0.2484,
      "step": 3180
    },
    {
      "epoch": 1.2046827794561934,
      "grad_norm": 6.806469917297363,
      "learning_rate": 1.577708006279435e-05,
      "loss": 0.3583,
      "step": 3190
    },
    {
      "epoch": 1.2084592145015105,
      "grad_norm": 10.420614242553711,
      "learning_rate": 1.5761381475667194e-05,
      "loss": 0.2568,
      "step": 3200
    },
    {
      "epoch": 1.2122356495468278,
      "grad_norm": 14.442121505737305,
      "learning_rate": 1.5745682888540033e-05,
      "loss": 0.2974,
      "step": 3210
    },
    {
      "epoch": 1.216012084592145,
      "grad_norm": 10.129316329956055,
      "learning_rate": 1.5729984301412876e-05,
      "loss": 0.3582,
      "step": 3220
    },
    {
      "epoch": 1.2197885196374623,
      "grad_norm": 12.106027603149414,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 0.3729,
      "step": 3230
    },
    {
      "epoch": 1.2235649546827794,
      "grad_norm": 5.699798583984375,
      "learning_rate": 1.5698587127158558e-05,
      "loss": 0.2958,
      "step": 3240
    },
    {
      "epoch": 1.2273413897280967,
      "grad_norm": 8.838249206542969,
      "learning_rate": 1.5682888540031398e-05,
      "loss": 0.3204,
      "step": 3250
    },
    {
      "epoch": 1.2311178247734138,
      "grad_norm": 4.469019412994385,
      "learning_rate": 1.566718995290424e-05,
      "loss": 0.1978,
      "step": 3260
    },
    {
      "epoch": 1.2348942598187311,
      "grad_norm": 3.9331257343292236,
      "learning_rate": 1.565149136577708e-05,
      "loss": 0.294,
      "step": 3270
    },
    {
      "epoch": 1.2386706948640485,
      "grad_norm": 5.648730278015137,
      "learning_rate": 1.5635792778649923e-05,
      "loss": 0.3937,
      "step": 3280
    },
    {
      "epoch": 1.2424471299093656,
      "grad_norm": 7.989498138427734,
      "learning_rate": 1.5620094191522762e-05,
      "loss": 0.3142,
      "step": 3290
    },
    {
      "epoch": 1.2462235649546827,
      "grad_norm": 11.611239433288574,
      "learning_rate": 1.5604395604395605e-05,
      "loss": 0.3945,
      "step": 3300
    },
    {
      "epoch": 1.25,
      "grad_norm": 8.421764373779297,
      "learning_rate": 1.5588697017268448e-05,
      "loss": 0.3804,
      "step": 3310
    },
    {
      "epoch": 1.2537764350453173,
      "grad_norm": 4.37327766418457,
      "learning_rate": 1.5572998430141287e-05,
      "loss": 0.3824,
      "step": 3320
    },
    {
      "epoch": 1.2575528700906344,
      "grad_norm": 7.509156227111816,
      "learning_rate": 1.555729984301413e-05,
      "loss": 0.4586,
      "step": 3330
    },
    {
      "epoch": 1.2613293051359515,
      "grad_norm": 5.080727577209473,
      "learning_rate": 1.5541601255886973e-05,
      "loss": 0.3256,
      "step": 3340
    },
    {
      "epoch": 1.2651057401812689,
      "grad_norm": 5.342507839202881,
      "learning_rate": 1.5525902668759812e-05,
      "loss": 0.3742,
      "step": 3350
    },
    {
      "epoch": 1.2688821752265862,
      "grad_norm": 9.937037467956543,
      "learning_rate": 1.5510204081632655e-05,
      "loss": 0.365,
      "step": 3360
    },
    {
      "epoch": 1.2726586102719033,
      "grad_norm": 3.6342742443084717,
      "learning_rate": 1.5494505494505498e-05,
      "loss": 0.2799,
      "step": 3370
    },
    {
      "epoch": 1.2764350453172204,
      "grad_norm": 7.953524112701416,
      "learning_rate": 1.5478806907378337e-05,
      "loss": 0.3467,
      "step": 3380
    },
    {
      "epoch": 1.2802114803625377,
      "grad_norm": 10.515580177307129,
      "learning_rate": 1.546310832025118e-05,
      "loss": 0.2485,
      "step": 3390
    },
    {
      "epoch": 1.283987915407855,
      "grad_norm": 4.897167682647705,
      "learning_rate": 1.544740973312402e-05,
      "loss": 0.3599,
      "step": 3400
    },
    {
      "epoch": 1.2877643504531722,
      "grad_norm": 4.759454250335693,
      "learning_rate": 1.5431711145996862e-05,
      "loss": 0.2067,
      "step": 3410
    },
    {
      "epoch": 1.2915407854984895,
      "grad_norm": 7.122422695159912,
      "learning_rate": 1.5416012558869705e-05,
      "loss": 0.3523,
      "step": 3420
    },
    {
      "epoch": 1.2953172205438066,
      "grad_norm": 3.242713212966919,
      "learning_rate": 1.5400313971742544e-05,
      "loss": 0.2948,
      "step": 3430
    },
    {
      "epoch": 1.299093655589124,
      "grad_norm": 5.7704901695251465,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.3622,
      "step": 3440
    },
    {
      "epoch": 1.302870090634441,
      "grad_norm": 4.635354995727539,
      "learning_rate": 1.5368916797488227e-05,
      "loss": 0.3218,
      "step": 3450
    },
    {
      "epoch": 1.3066465256797584,
      "grad_norm": 6.294947624206543,
      "learning_rate": 1.535321821036107e-05,
      "loss": 0.3096,
      "step": 3460
    },
    {
      "epoch": 1.3104229607250755,
      "grad_norm": 7.4496965408325195,
      "learning_rate": 1.533751962323391e-05,
      "loss": 0.393,
      "step": 3470
    },
    {
      "epoch": 1.3141993957703928,
      "grad_norm": 10.106374740600586,
      "learning_rate": 1.532182103610675e-05,
      "loss": 0.3105,
      "step": 3480
    },
    {
      "epoch": 1.31797583081571,
      "grad_norm": 4.0826802253723145,
      "learning_rate": 1.530612244897959e-05,
      "loss": 0.3271,
      "step": 3490
    },
    {
      "epoch": 1.3217522658610272,
      "grad_norm": 8.083657264709473,
      "learning_rate": 1.5290423861852434e-05,
      "loss": 0.3309,
      "step": 3500
    },
    {
      "epoch": 1.3255287009063443,
      "grad_norm": 5.881535530090332,
      "learning_rate": 1.5274725274725277e-05,
      "loss": 0.3276,
      "step": 3510
    },
    {
      "epoch": 1.3293051359516617,
      "grad_norm": 5.751142501831055,
      "learning_rate": 1.5259026687598116e-05,
      "loss": 0.4294,
      "step": 3520
    },
    {
      "epoch": 1.3330815709969788,
      "grad_norm": 3.918837070465088,
      "learning_rate": 1.524332810047096e-05,
      "loss": 0.3367,
      "step": 3530
    },
    {
      "epoch": 1.336858006042296,
      "grad_norm": 9.468428611755371,
      "learning_rate": 1.52276295133438e-05,
      "loss": 0.3366,
      "step": 3540
    },
    {
      "epoch": 1.3406344410876132,
      "grad_norm": 6.646855354309082,
      "learning_rate": 1.5211930926216643e-05,
      "loss": 0.3521,
      "step": 3550
    },
    {
      "epoch": 1.3444108761329305,
      "grad_norm": 7.002984523773193,
      "learning_rate": 1.5196232339089482e-05,
      "loss": 0.3483,
      "step": 3560
    },
    {
      "epoch": 1.3481873111782479,
      "grad_norm": 6.172829627990723,
      "learning_rate": 1.5180533751962325e-05,
      "loss": 0.2698,
      "step": 3570
    },
    {
      "epoch": 1.351963746223565,
      "grad_norm": 10.674654006958008,
      "learning_rate": 1.5164835164835166e-05,
      "loss": 0.3557,
      "step": 3580
    },
    {
      "epoch": 1.355740181268882,
      "grad_norm": 8.633023262023926,
      "learning_rate": 1.5149136577708007e-05,
      "loss": 0.3619,
      "step": 3590
    },
    {
      "epoch": 1.3595166163141994,
      "grad_norm": 7.977505207061768,
      "learning_rate": 1.5133437990580848e-05,
      "loss": 0.3348,
      "step": 3600
    },
    {
      "epoch": 1.3632930513595167,
      "grad_norm": 8.178815841674805,
      "learning_rate": 1.5117739403453691e-05,
      "loss": 0.2921,
      "step": 3610
    },
    {
      "epoch": 1.3670694864048338,
      "grad_norm": 2.0745108127593994,
      "learning_rate": 1.510204081632653e-05,
      "loss": 0.3448,
      "step": 3620
    },
    {
      "epoch": 1.370845921450151,
      "grad_norm": 16.21959686279297,
      "learning_rate": 1.5086342229199373e-05,
      "loss": 0.3235,
      "step": 3630
    },
    {
      "epoch": 1.3746223564954683,
      "grad_norm": 8.348544120788574,
      "learning_rate": 1.5070643642072216e-05,
      "loss": 0.2742,
      "step": 3640
    },
    {
      "epoch": 1.3783987915407856,
      "grad_norm": 11.241308212280273,
      "learning_rate": 1.5054945054945056e-05,
      "loss": 0.3393,
      "step": 3650
    },
    {
      "epoch": 1.3821752265861027,
      "grad_norm": 6.202080249786377,
      "learning_rate": 1.5039246467817898e-05,
      "loss": 0.3526,
      "step": 3660
    },
    {
      "epoch": 1.3859516616314198,
      "grad_norm": 11.50708293914795,
      "learning_rate": 1.5023547880690738e-05,
      "loss": 0.2409,
      "step": 3670
    },
    {
      "epoch": 1.3897280966767371,
      "grad_norm": 7.710814952850342,
      "learning_rate": 1.500784929356358e-05,
      "loss": 0.317,
      "step": 3680
    },
    {
      "epoch": 1.3935045317220545,
      "grad_norm": 9.903047561645508,
      "learning_rate": 1.4992150706436422e-05,
      "loss": 0.37,
      "step": 3690
    },
    {
      "epoch": 1.3972809667673716,
      "grad_norm": 11.704237937927246,
      "learning_rate": 1.4976452119309265e-05,
      "loss": 0.244,
      "step": 3700
    },
    {
      "epoch": 1.401057401812689,
      "grad_norm": 8.983576774597168,
      "learning_rate": 1.4960753532182104e-05,
      "loss": 0.333,
      "step": 3710
    },
    {
      "epoch": 1.404833836858006,
      "grad_norm": 11.083332061767578,
      "learning_rate": 1.4945054945054947e-05,
      "loss": 0.3491,
      "step": 3720
    },
    {
      "epoch": 1.4086102719033233,
      "grad_norm": 5.163478374481201,
      "learning_rate": 1.4929356357927786e-05,
      "loss": 0.348,
      "step": 3730
    },
    {
      "epoch": 1.4123867069486404,
      "grad_norm": 9.922869682312012,
      "learning_rate": 1.4913657770800629e-05,
      "loss": 0.2245,
      "step": 3740
    },
    {
      "epoch": 1.4161631419939578,
      "grad_norm": 14.230606079101562,
      "learning_rate": 1.4897959183673472e-05,
      "loss": 0.3336,
      "step": 3750
    },
    {
      "epoch": 1.4199395770392749,
      "grad_norm": 9.840131759643555,
      "learning_rate": 1.4882260596546311e-05,
      "loss": 0.4675,
      "step": 3760
    },
    {
      "epoch": 1.4237160120845922,
      "grad_norm": 6.302831649780273,
      "learning_rate": 1.4866562009419154e-05,
      "loss": 0.3375,
      "step": 3770
    },
    {
      "epoch": 1.4274924471299093,
      "grad_norm": 3.2473576068878174,
      "learning_rate": 1.4850863422291995e-05,
      "loss": 0.318,
      "step": 3780
    },
    {
      "epoch": 1.4312688821752266,
      "grad_norm": 7.458140850067139,
      "learning_rate": 1.4835164835164836e-05,
      "loss": 0.3613,
      "step": 3790
    },
    {
      "epoch": 1.4350453172205437,
      "grad_norm": 6.979550361633301,
      "learning_rate": 1.4819466248037677e-05,
      "loss": 0.3261,
      "step": 3800
    },
    {
      "epoch": 1.438821752265861,
      "grad_norm": 4.344079494476318,
      "learning_rate": 1.480376766091052e-05,
      "loss": 0.2475,
      "step": 3810
    },
    {
      "epoch": 1.4425981873111782,
      "grad_norm": 6.417785167694092,
      "learning_rate": 1.478806907378336e-05,
      "loss": 0.3922,
      "step": 3820
    },
    {
      "epoch": 1.4463746223564955,
      "grad_norm": 7.273554801940918,
      "learning_rate": 1.4772370486656202e-05,
      "loss": 0.2635,
      "step": 3830
    },
    {
      "epoch": 1.4501510574018126,
      "grad_norm": 8.855433464050293,
      "learning_rate": 1.4756671899529042e-05,
      "loss": 0.2749,
      "step": 3840
    },
    {
      "epoch": 1.45392749244713,
      "grad_norm": 8.855847358703613,
      "learning_rate": 1.4740973312401885e-05,
      "loss": 0.3958,
      "step": 3850
    },
    {
      "epoch": 1.4577039274924473,
      "grad_norm": 4.873955249786377,
      "learning_rate": 1.4725274725274727e-05,
      "loss": 0.3086,
      "step": 3860
    },
    {
      "epoch": 1.4614803625377644,
      "grad_norm": 13.335678100585938,
      "learning_rate": 1.4709576138147567e-05,
      "loss": 0.3567,
      "step": 3870
    },
    {
      "epoch": 1.4652567975830815,
      "grad_norm": 9.441472053527832,
      "learning_rate": 1.469387755102041e-05,
      "loss": 0.3347,
      "step": 3880
    },
    {
      "epoch": 1.4690332326283988,
      "grad_norm": 4.500752925872803,
      "learning_rate": 1.467817896389325e-05,
      "loss": 0.3658,
      "step": 3890
    },
    {
      "epoch": 1.4728096676737161,
      "grad_norm": 9.292254447937012,
      "learning_rate": 1.4662480376766094e-05,
      "loss": 0.3701,
      "step": 3900
    },
    {
      "epoch": 1.4765861027190332,
      "grad_norm": 3.6381781101226807,
      "learning_rate": 1.4646781789638933e-05,
      "loss": 0.3579,
      "step": 3910
    },
    {
      "epoch": 1.4803625377643503,
      "grad_norm": 5.993612289428711,
      "learning_rate": 1.4631083202511776e-05,
      "loss": 0.3451,
      "step": 3920
    },
    {
      "epoch": 1.4841389728096677,
      "grad_norm": 10.055490493774414,
      "learning_rate": 1.4615384615384615e-05,
      "loss": 0.3562,
      "step": 3930
    },
    {
      "epoch": 1.487915407854985,
      "grad_norm": 4.145695686340332,
      "learning_rate": 1.4599686028257458e-05,
      "loss": 0.3738,
      "step": 3940
    },
    {
      "epoch": 1.491691842900302,
      "grad_norm": 11.569295883178711,
      "learning_rate": 1.45839874411303e-05,
      "loss": 0.2749,
      "step": 3950
    },
    {
      "epoch": 1.4954682779456192,
      "grad_norm": 7.212658405303955,
      "learning_rate": 1.456828885400314e-05,
      "loss": 0.3055,
      "step": 3960
    },
    {
      "epoch": 1.4992447129909365,
      "grad_norm": 7.109504222869873,
      "learning_rate": 1.4552590266875983e-05,
      "loss": 0.3581,
      "step": 3970
    },
    {
      "epoch": 1.5030211480362539,
      "grad_norm": 8.069272994995117,
      "learning_rate": 1.4536891679748824e-05,
      "loss": 0.3863,
      "step": 3980
    },
    {
      "epoch": 1.506797583081571,
      "grad_norm": 9.553424835205078,
      "learning_rate": 1.4521193092621665e-05,
      "loss": 0.3277,
      "step": 3990
    },
    {
      "epoch": 1.510574018126888,
      "grad_norm": 3.8149449825286865,
      "learning_rate": 1.4505494505494506e-05,
      "loss": 0.2951,
      "step": 4000
    },
    {
      "epoch": 1.5143504531722054,
      "grad_norm": 5.395859718322754,
      "learning_rate": 1.448979591836735e-05,
      "loss": 0.2388,
      "step": 4010
    },
    {
      "epoch": 1.5181268882175227,
      "grad_norm": 11.225834846496582,
      "learning_rate": 1.4474097331240189e-05,
      "loss": 0.3427,
      "step": 4020
    },
    {
      "epoch": 1.5219033232628398,
      "grad_norm": 3.736187219619751,
      "learning_rate": 1.4458398744113031e-05,
      "loss": 0.2307,
      "step": 4030
    },
    {
      "epoch": 1.525679758308157,
      "grad_norm": 2.5575389862060547,
      "learning_rate": 1.4442700156985871e-05,
      "loss": 0.3797,
      "step": 4040
    },
    {
      "epoch": 1.5294561933534743,
      "grad_norm": 13.402770042419434,
      "learning_rate": 1.4427001569858714e-05,
      "loss": 0.4574,
      "step": 4050
    },
    {
      "epoch": 1.5332326283987916,
      "grad_norm": 10.03855037689209,
      "learning_rate": 1.4411302982731555e-05,
      "loss": 0.4216,
      "step": 4060
    },
    {
      "epoch": 1.537009063444109,
      "grad_norm": 5.616289138793945,
      "learning_rate": 1.4395604395604396e-05,
      "loss": 0.349,
      "step": 4070
    },
    {
      "epoch": 1.540785498489426,
      "grad_norm": 7.311866283416748,
      "learning_rate": 1.4379905808477239e-05,
      "loss": 0.3124,
      "step": 4080
    },
    {
      "epoch": 1.5445619335347431,
      "grad_norm": 11.989387512207031,
      "learning_rate": 1.436420722135008e-05,
      "loss": 0.3393,
      "step": 4090
    },
    {
      "epoch": 1.5483383685800605,
      "grad_norm": 5.647732734680176,
      "learning_rate": 1.4348508634222923e-05,
      "loss": 0.2959,
      "step": 4100
    },
    {
      "epoch": 1.5521148036253778,
      "grad_norm": 8.906438827514648,
      "learning_rate": 1.4332810047095762e-05,
      "loss": 0.3164,
      "step": 4110
    },
    {
      "epoch": 1.555891238670695,
      "grad_norm": 9.68625259399414,
      "learning_rate": 1.4317111459968605e-05,
      "loss": 0.3795,
      "step": 4120
    },
    {
      "epoch": 1.559667673716012,
      "grad_norm": 4.1534247398376465,
      "learning_rate": 1.4301412872841444e-05,
      "loss": 0.3393,
      "step": 4130
    },
    {
      "epoch": 1.5634441087613293,
      "grad_norm": 2.9878034591674805,
      "learning_rate": 1.4285714285714287e-05,
      "loss": 0.2849,
      "step": 4140
    },
    {
      "epoch": 1.5672205438066467,
      "grad_norm": 7.480381488800049,
      "learning_rate": 1.4270015698587128e-05,
      "loss": 0.3242,
      "step": 4150
    },
    {
      "epoch": 1.5709969788519638,
      "grad_norm": 4.697212219238281,
      "learning_rate": 1.425431711145997e-05,
      "loss": 0.339,
      "step": 4160
    },
    {
      "epoch": 1.5747734138972809,
      "grad_norm": 7.061237335205078,
      "learning_rate": 1.423861852433281e-05,
      "loss": 0.3213,
      "step": 4170
    },
    {
      "epoch": 1.5785498489425982,
      "grad_norm": 7.434317588806152,
      "learning_rate": 1.4222919937205653e-05,
      "loss": 0.3343,
      "step": 4180
    },
    {
      "epoch": 1.5823262839879155,
      "grad_norm": 9.288551330566406,
      "learning_rate": 1.4207221350078494e-05,
      "loss": 0.3322,
      "step": 4190
    },
    {
      "epoch": 1.5861027190332326,
      "grad_norm": 7.608153343200684,
      "learning_rate": 1.4191522762951335e-05,
      "loss": 0.3355,
      "step": 4200
    },
    {
      "epoch": 1.5898791540785497,
      "grad_norm": 8.290657043457031,
      "learning_rate": 1.4175824175824178e-05,
      "loss": 0.2837,
      "step": 4210
    },
    {
      "epoch": 1.593655589123867,
      "grad_norm": 6.377478122711182,
      "learning_rate": 1.4160125588697018e-05,
      "loss": 0.2877,
      "step": 4220
    },
    {
      "epoch": 1.5974320241691844,
      "grad_norm": 4.5460734367370605,
      "learning_rate": 1.414442700156986e-05,
      "loss": 0.2557,
      "step": 4230
    },
    {
      "epoch": 1.6012084592145015,
      "grad_norm": 6.5593743324279785,
      "learning_rate": 1.41287284144427e-05,
      "loss": 0.3298,
      "step": 4240
    },
    {
      "epoch": 1.6049848942598186,
      "grad_norm": 6.975845813751221,
      "learning_rate": 1.4113029827315543e-05,
      "loss": 0.3431,
      "step": 4250
    },
    {
      "epoch": 1.608761329305136,
      "grad_norm": 8.398362159729004,
      "learning_rate": 1.4097331240188384e-05,
      "loss": 0.3551,
      "step": 4260
    },
    {
      "epoch": 1.6125377643504533,
      "grad_norm": 8.375420570373535,
      "learning_rate": 1.4081632653061225e-05,
      "loss": 0.3156,
      "step": 4270
    },
    {
      "epoch": 1.6163141993957704,
      "grad_norm": 4.68458366394043,
      "learning_rate": 1.4065934065934068e-05,
      "loss": 0.3261,
      "step": 4280
    },
    {
      "epoch": 1.6200906344410875,
      "grad_norm": 3.364830493927002,
      "learning_rate": 1.4050235478806909e-05,
      "loss": 0.3292,
      "step": 4290
    },
    {
      "epoch": 1.6238670694864048,
      "grad_norm": 5.094577312469482,
      "learning_rate": 1.403453689167975e-05,
      "loss": 0.2482,
      "step": 4300
    },
    {
      "epoch": 1.6276435045317221,
      "grad_norm": 9.164958953857422,
      "learning_rate": 1.4018838304552591e-05,
      "loss": 0.2893,
      "step": 4310
    },
    {
      "epoch": 1.6314199395770392,
      "grad_norm": 9.235696792602539,
      "learning_rate": 1.4003139717425434e-05,
      "loss": 0.5157,
      "step": 4320
    },
    {
      "epoch": 1.6351963746223563,
      "grad_norm": 9.064032554626465,
      "learning_rate": 1.3987441130298273e-05,
      "loss": 0.3647,
      "step": 4330
    },
    {
      "epoch": 1.6389728096676737,
      "grad_norm": 4.973121643066406,
      "learning_rate": 1.3971742543171116e-05,
      "loss": 0.3031,
      "step": 4340
    },
    {
      "epoch": 1.642749244712991,
      "grad_norm": 5.599598407745361,
      "learning_rate": 1.3956043956043957e-05,
      "loss": 0.3664,
      "step": 4350
    },
    {
      "epoch": 1.646525679758308,
      "grad_norm": 3.5802268981933594,
      "learning_rate": 1.3940345368916798e-05,
      "loss": 0.3026,
      "step": 4360
    },
    {
      "epoch": 1.6503021148036254,
      "grad_norm": 4.858715534210205,
      "learning_rate": 1.392464678178964e-05,
      "loss": 0.3928,
      "step": 4370
    },
    {
      "epoch": 1.6540785498489425,
      "grad_norm": 5.422966003417969,
      "learning_rate": 1.3908948194662482e-05,
      "loss": 0.3332,
      "step": 4380
    },
    {
      "epoch": 1.6578549848942599,
      "grad_norm": 4.653145790100098,
      "learning_rate": 1.3893249607535323e-05,
      "loss": 0.3575,
      "step": 4390
    },
    {
      "epoch": 1.6616314199395772,
      "grad_norm": 3.3595077991485596,
      "learning_rate": 1.3877551020408165e-05,
      "loss": 0.3216,
      "step": 4400
    },
    {
      "epoch": 1.6654078549848943,
      "grad_norm": 8.152122497558594,
      "learning_rate": 1.3861852433281007e-05,
      "loss": 0.3618,
      "step": 4410
    },
    {
      "epoch": 1.6691842900302114,
      "grad_norm": 4.965376853942871,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 0.3496,
      "step": 4420
    },
    {
      "epoch": 1.6729607250755287,
      "grad_norm": 8.497936248779297,
      "learning_rate": 1.383045525902669e-05,
      "loss": 0.4077,
      "step": 4430
    },
    {
      "epoch": 1.676737160120846,
      "grad_norm": 8.749063491821289,
      "learning_rate": 1.3814756671899529e-05,
      "loss": 0.3065,
      "step": 4440
    },
    {
      "epoch": 1.6805135951661632,
      "grad_norm": 7.7762980461120605,
      "learning_rate": 1.3799058084772372e-05,
      "loss": 0.3608,
      "step": 4450
    },
    {
      "epoch": 1.6842900302114803,
      "grad_norm": 9.497091293334961,
      "learning_rate": 1.3783359497645213e-05,
      "loss": 0.2697,
      "step": 4460
    },
    {
      "epoch": 1.6880664652567976,
      "grad_norm": 6.012354373931885,
      "learning_rate": 1.3767660910518054e-05,
      "loss": 0.3574,
      "step": 4470
    },
    {
      "epoch": 1.691842900302115,
      "grad_norm": 4.025749206542969,
      "learning_rate": 1.3751962323390895e-05,
      "loss": 0.3826,
      "step": 4480
    },
    {
      "epoch": 1.695619335347432,
      "grad_norm": 4.418907642364502,
      "learning_rate": 1.3736263736263738e-05,
      "loss": 0.2898,
      "step": 4490
    },
    {
      "epoch": 1.6993957703927491,
      "grad_norm": 10.436244010925293,
      "learning_rate": 1.3720565149136579e-05,
      "loss": 0.325,
      "step": 4500
    },
    {
      "epoch": 1.7031722054380665,
      "grad_norm": 18.243181228637695,
      "learning_rate": 1.370486656200942e-05,
      "loss": 0.3345,
      "step": 4510
    },
    {
      "epoch": 1.7069486404833838,
      "grad_norm": 9.23857593536377,
      "learning_rate": 1.3689167974882263e-05,
      "loss": 0.3628,
      "step": 4520
    },
    {
      "epoch": 1.710725075528701,
      "grad_norm": 6.9798264503479,
      "learning_rate": 1.3673469387755102e-05,
      "loss": 0.3332,
      "step": 4530
    },
    {
      "epoch": 1.714501510574018,
      "grad_norm": 9.27995491027832,
      "learning_rate": 1.3657770800627945e-05,
      "loss": 0.3078,
      "step": 4540
    },
    {
      "epoch": 1.7182779456193353,
      "grad_norm": 8.056805610656738,
      "learning_rate": 1.3642072213500786e-05,
      "loss": 0.2663,
      "step": 4550
    },
    {
      "epoch": 1.7220543806646527,
      "grad_norm": 8.388625144958496,
      "learning_rate": 1.3626373626373627e-05,
      "loss": 0.3139,
      "step": 4560
    },
    {
      "epoch": 1.7258308157099698,
      "grad_norm": 9.647377014160156,
      "learning_rate": 1.3610675039246469e-05,
      "loss": 0.3099,
      "step": 4570
    },
    {
      "epoch": 1.7296072507552869,
      "grad_norm": 12.647250175476074,
      "learning_rate": 1.3594976452119311e-05,
      "loss": 0.3321,
      "step": 4580
    },
    {
      "epoch": 1.7333836858006042,
      "grad_norm": 4.200820446014404,
      "learning_rate": 1.357927786499215e-05,
      "loss": 0.2166,
      "step": 4590
    },
    {
      "epoch": 1.7371601208459215,
      "grad_norm": 5.45413875579834,
      "learning_rate": 1.3563579277864994e-05,
      "loss": 0.3823,
      "step": 4600
    },
    {
      "epoch": 1.7409365558912386,
      "grad_norm": 8.506966590881348,
      "learning_rate": 1.3547880690737836e-05,
      "loss": 0.2216,
      "step": 4610
    },
    {
      "epoch": 1.7447129909365557,
      "grad_norm": 9.415560722351074,
      "learning_rate": 1.3532182103610676e-05,
      "loss": 0.3183,
      "step": 4620
    },
    {
      "epoch": 1.748489425981873,
      "grad_norm": 5.489542007446289,
      "learning_rate": 1.3516483516483519e-05,
      "loss": 0.3275,
      "step": 4630
    },
    {
      "epoch": 1.7522658610271904,
      "grad_norm": 5.8505449295043945,
      "learning_rate": 1.3500784929356358e-05,
      "loss": 0.3007,
      "step": 4640
    },
    {
      "epoch": 1.7560422960725075,
      "grad_norm": 6.675160884857178,
      "learning_rate": 1.34850863422292e-05,
      "loss": 0.3237,
      "step": 4650
    },
    {
      "epoch": 1.7598187311178246,
      "grad_norm": 5.373905181884766,
      "learning_rate": 1.3469387755102042e-05,
      "loss": 0.2686,
      "step": 4660
    },
    {
      "epoch": 1.763595166163142,
      "grad_norm": 6.102963924407959,
      "learning_rate": 1.3453689167974883e-05,
      "loss": 0.3066,
      "step": 4670
    },
    {
      "epoch": 1.7673716012084593,
      "grad_norm": 3.3556387424468994,
      "learning_rate": 1.3437990580847724e-05,
      "loss": 0.3039,
      "step": 4680
    },
    {
      "epoch": 1.7711480362537766,
      "grad_norm": 7.047527313232422,
      "learning_rate": 1.3422291993720567e-05,
      "loss": 0.353,
      "step": 4690
    },
    {
      "epoch": 1.7749244712990937,
      "grad_norm": 6.501294136047363,
      "learning_rate": 1.3406593406593406e-05,
      "loss": 0.3823,
      "step": 4700
    },
    {
      "epoch": 1.7787009063444108,
      "grad_norm": 11.426487922668457,
      "learning_rate": 1.339089481946625e-05,
      "loss": 0.3,
      "step": 4710
    },
    {
      "epoch": 1.7824773413897281,
      "grad_norm": 6.271367073059082,
      "learning_rate": 1.3375196232339092e-05,
      "loss": 0.3741,
      "step": 4720
    },
    {
      "epoch": 1.7862537764350455,
      "grad_norm": 6.819406032562256,
      "learning_rate": 1.3359497645211931e-05,
      "loss": 0.3402,
      "step": 4730
    },
    {
      "epoch": 1.7900302114803626,
      "grad_norm": 5.728219032287598,
      "learning_rate": 1.3343799058084774e-05,
      "loss": 0.3172,
      "step": 4740
    },
    {
      "epoch": 1.7938066465256797,
      "grad_norm": 13.849563598632812,
      "learning_rate": 1.3328100470957614e-05,
      "loss": 0.2908,
      "step": 4750
    },
    {
      "epoch": 1.797583081570997,
      "grad_norm": 8.083516120910645,
      "learning_rate": 1.3312401883830456e-05,
      "loss": 0.4071,
      "step": 4760
    },
    {
      "epoch": 1.8013595166163143,
      "grad_norm": 8.046614646911621,
      "learning_rate": 1.3296703296703298e-05,
      "loss": 0.3388,
      "step": 4770
    },
    {
      "epoch": 1.8051359516616314,
      "grad_norm": 8.065943717956543,
      "learning_rate": 1.328100470957614e-05,
      "loss": 0.3336,
      "step": 4780
    },
    {
      "epoch": 1.8089123867069485,
      "grad_norm": 3.8308207988739014,
      "learning_rate": 1.326530612244898e-05,
      "loss": 0.3791,
      "step": 4790
    },
    {
      "epoch": 1.8126888217522659,
      "grad_norm": 7.696929454803467,
      "learning_rate": 1.3249607535321823e-05,
      "loss": 0.2417,
      "step": 4800
    },
    {
      "epoch": 1.8164652567975832,
      "grad_norm": 13.242650032043457,
      "learning_rate": 1.3233908948194662e-05,
      "loss": 0.2785,
      "step": 4810
    },
    {
      "epoch": 1.8202416918429003,
      "grad_norm": 7.076053619384766,
      "learning_rate": 1.3218210361067505e-05,
      "loss": 0.2834,
      "step": 4820
    },
    {
      "epoch": 1.8240181268882174,
      "grad_norm": 5.774463653564453,
      "learning_rate": 1.3202511773940348e-05,
      "loss": 0.375,
      "step": 4830
    },
    {
      "epoch": 1.8277945619335347,
      "grad_norm": 15.487221717834473,
      "learning_rate": 1.3186813186813187e-05,
      "loss": 0.3154,
      "step": 4840
    },
    {
      "epoch": 1.831570996978852,
      "grad_norm": 6.7832255363464355,
      "learning_rate": 1.317111459968603e-05,
      "loss": 0.3098,
      "step": 4850
    },
    {
      "epoch": 1.8353474320241692,
      "grad_norm": 3.1786186695098877,
      "learning_rate": 1.3155416012558871e-05,
      "loss": 0.3416,
      "step": 4860
    },
    {
      "epoch": 1.8391238670694863,
      "grad_norm": 8.670178413391113,
      "learning_rate": 1.3139717425431712e-05,
      "loss": 0.3648,
      "step": 4870
    },
    {
      "epoch": 1.8429003021148036,
      "grad_norm": 5.099954128265381,
      "learning_rate": 1.3124018838304553e-05,
      "loss": 0.4387,
      "step": 4880
    },
    {
      "epoch": 1.846676737160121,
      "grad_norm": 6.75115966796875,
      "learning_rate": 1.3108320251177396e-05,
      "loss": 0.3309,
      "step": 4890
    },
    {
      "epoch": 1.850453172205438,
      "grad_norm": 28.570451736450195,
      "learning_rate": 1.3092621664050235e-05,
      "loss": 0.2917,
      "step": 4900
    },
    {
      "epoch": 1.8542296072507551,
      "grad_norm": 3.0209405422210693,
      "learning_rate": 1.3076923076923078e-05,
      "loss": 0.3068,
      "step": 4910
    },
    {
      "epoch": 1.8580060422960725,
      "grad_norm": 7.155261993408203,
      "learning_rate": 1.3061224489795918e-05,
      "loss": 0.3365,
      "step": 4920
    },
    {
      "epoch": 1.8617824773413898,
      "grad_norm": 6.559893608093262,
      "learning_rate": 1.304552590266876e-05,
      "loss": 0.2964,
      "step": 4930
    },
    {
      "epoch": 1.865558912386707,
      "grad_norm": 8.275884628295898,
      "learning_rate": 1.3029827315541603e-05,
      "loss": 0.2919,
      "step": 4940
    },
    {
      "epoch": 1.869335347432024,
      "grad_norm": 8.337692260742188,
      "learning_rate": 1.3014128728414443e-05,
      "loss": 0.342,
      "step": 4950
    },
    {
      "epoch": 1.8731117824773413,
      "grad_norm": 7.486969470977783,
      "learning_rate": 1.2998430141287286e-05,
      "loss": 0.3202,
      "step": 4960
    },
    {
      "epoch": 1.8768882175226587,
      "grad_norm": 3.0602779388427734,
      "learning_rate": 1.2982731554160127e-05,
      "loss": 0.3563,
      "step": 4970
    },
    {
      "epoch": 1.880664652567976,
      "grad_norm": 6.878835201263428,
      "learning_rate": 1.296703296703297e-05,
      "loss": 0.3867,
      "step": 4980
    },
    {
      "epoch": 1.884441087613293,
      "grad_norm": 7.219170570373535,
      "learning_rate": 1.2951334379905809e-05,
      "loss": 0.3506,
      "step": 4990
    },
    {
      "epoch": 1.8882175226586102,
      "grad_norm": 3.105236291885376,
      "learning_rate": 1.2935635792778652e-05,
      "loss": 0.2661,
      "step": 5000
    },
    {
      "epoch": 1.8919939577039275,
      "grad_norm": 6.111880302429199,
      "learning_rate": 1.2919937205651491e-05,
      "loss": 0.3667,
      "step": 5010
    },
    {
      "epoch": 1.8957703927492449,
      "grad_norm": 7.55548095703125,
      "learning_rate": 1.2904238618524334e-05,
      "loss": 0.2826,
      "step": 5020
    },
    {
      "epoch": 1.899546827794562,
      "grad_norm": 7.208579063415527,
      "learning_rate": 1.2888540031397175e-05,
      "loss": 0.3094,
      "step": 5030
    },
    {
      "epoch": 1.903323262839879,
      "grad_norm": 4.587603569030762,
      "learning_rate": 1.2872841444270016e-05,
      "loss": 0.3327,
      "step": 5040
    },
    {
      "epoch": 1.9070996978851964,
      "grad_norm": 13.393630981445312,
      "learning_rate": 1.2857142857142859e-05,
      "loss": 0.328,
      "step": 5050
    },
    {
      "epoch": 1.9108761329305137,
      "grad_norm": 4.570909023284912,
      "learning_rate": 1.28414442700157e-05,
      "loss": 0.3128,
      "step": 5060
    },
    {
      "epoch": 1.9146525679758308,
      "grad_norm": 9.83387279510498,
      "learning_rate": 1.2825745682888541e-05,
      "loss": 0.3292,
      "step": 5070
    },
    {
      "epoch": 1.918429003021148,
      "grad_norm": 9.24466323852539,
      "learning_rate": 1.2810047095761382e-05,
      "loss": 0.4077,
      "step": 5080
    },
    {
      "epoch": 1.9222054380664653,
      "grad_norm": 8.329588890075684,
      "learning_rate": 1.2794348508634225e-05,
      "loss": 0.3645,
      "step": 5090
    },
    {
      "epoch": 1.9259818731117826,
      "grad_norm": 7.113430976867676,
      "learning_rate": 1.2778649921507064e-05,
      "loss": 0.2678,
      "step": 5100
    },
    {
      "epoch": 1.9297583081570997,
      "grad_norm": 11.764229774475098,
      "learning_rate": 1.2762951334379907e-05,
      "loss": 0.3026,
      "step": 5110
    },
    {
      "epoch": 1.9335347432024168,
      "grad_norm": 4.874226093292236,
      "learning_rate": 1.2747252747252747e-05,
      "loss": 0.3542,
      "step": 5120
    },
    {
      "epoch": 1.9373111782477341,
      "grad_norm": 7.180064678192139,
      "learning_rate": 1.273155416012559e-05,
      "loss": 0.3143,
      "step": 5130
    },
    {
      "epoch": 1.9410876132930515,
      "grad_norm": 6.414984703063965,
      "learning_rate": 1.271585557299843e-05,
      "loss": 0.3775,
      "step": 5140
    },
    {
      "epoch": 1.9448640483383686,
      "grad_norm": 7.018163204193115,
      "learning_rate": 1.2700156985871272e-05,
      "loss": 0.3204,
      "step": 5150
    },
    {
      "epoch": 1.9486404833836857,
      "grad_norm": 3.9247217178344727,
      "learning_rate": 1.2684458398744115e-05,
      "loss": 0.2406,
      "step": 5160
    },
    {
      "epoch": 1.952416918429003,
      "grad_norm": 8.351943969726562,
      "learning_rate": 1.2668759811616956e-05,
      "loss": 0.3145,
      "step": 5170
    },
    {
      "epoch": 1.9561933534743203,
      "grad_norm": 9.636531829833984,
      "learning_rate": 1.2653061224489798e-05,
      "loss": 0.3675,
      "step": 5180
    },
    {
      "epoch": 1.9599697885196374,
      "grad_norm": 7.729325294494629,
      "learning_rate": 1.2637362637362638e-05,
      "loss": 0.3674,
      "step": 5190
    },
    {
      "epoch": 1.9637462235649545,
      "grad_norm": 5.345048904418945,
      "learning_rate": 1.262166405023548e-05,
      "loss": 0.3218,
      "step": 5200
    },
    {
      "epoch": 1.9675226586102719,
      "grad_norm": 2.609553098678589,
      "learning_rate": 1.260596546310832e-05,
      "loss": 0.2786,
      "step": 5210
    },
    {
      "epoch": 1.9712990936555892,
      "grad_norm": 2.7784624099731445,
      "learning_rate": 1.2590266875981163e-05,
      "loss": 0.2991,
      "step": 5220
    },
    {
      "epoch": 1.9750755287009063,
      "grad_norm": 4.181586742401123,
      "learning_rate": 1.2574568288854004e-05,
      "loss": 0.3069,
      "step": 5230
    },
    {
      "epoch": 1.9788519637462234,
      "grad_norm": 6.965152740478516,
      "learning_rate": 1.2558869701726845e-05,
      "loss": 0.3801,
      "step": 5240
    },
    {
      "epoch": 1.9826283987915407,
      "grad_norm": 4.699124813079834,
      "learning_rate": 1.2543171114599686e-05,
      "loss": 0.3365,
      "step": 5250
    },
    {
      "epoch": 1.986404833836858,
      "grad_norm": 4.841784954071045,
      "learning_rate": 1.2527472527472529e-05,
      "loss": 0.4487,
      "step": 5260
    },
    {
      "epoch": 1.9901812688821754,
      "grad_norm": 4.72258186340332,
      "learning_rate": 1.251177394034537e-05,
      "loss": 0.3641,
      "step": 5270
    },
    {
      "epoch": 1.9939577039274925,
      "grad_norm": 2.766127347946167,
      "learning_rate": 1.2496075353218211e-05,
      "loss": 0.3584,
      "step": 5280
    },
    {
      "epoch": 1.9977341389728096,
      "grad_norm": 5.263525485992432,
      "learning_rate": 1.2480376766091054e-05,
      "loss": 0.4047,
      "step": 5290
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8326565303616961,
      "eval_f1": 0.7928454524199205,
      "eval_loss": 0.3861248195171356,
      "eval_precision": 0.802603550295858,
      "eval_recall": 0.7833217833217834,
      "eval_runtime": 50.0016,
      "eval_samples_per_second": 211.773,
      "eval_steps_per_second": 3.32,
      "step": 5296
    }
  ],
  "logging_steps": 10,
  "max_steps": 13240,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5572165930414080.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
